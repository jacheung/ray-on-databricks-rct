{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cdb2def-1b55-47b5-a2f5-3172cc00c96a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install ray[all] lancedb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "938d0e31-c19b-4c5a-b941-f649074c66f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "# Connect to global ray cluster\n",
    "ray.init(_node_ip_address='10.139.64.164')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c72e272b-49b6-4389-a4c7-59e5d2a6b722",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "catalog_name = 'jon_cheung'\n",
    "schema_name = 'vizio_poc'\n",
    "lance_table_name = 'audio_100M_chunk_shard_5'\n",
    "num_test_vectors = 5_000\n",
    "\n",
    "\n",
    "audio_parquet_path = f'/Volumes/{catalog_name}/{schema_name}/{lance_table_name}'\n",
    "pyarrow_schema = pa.schema(\n",
    "    [\n",
    "        pa.field(\"id\", pa.int64()),\n",
    "        pa.field(\"list_col\", pa.list_(pa.float16(), 35)),   # Fixed size list\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcc0fd12-2973-4a8b-b068-3ecc64d24e3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pyarrow.dataset as ds\n",
    "import lancedb\n",
    "from tqdm import tqdm\n",
    "\n",
    "num_closest_vectors = 1\n",
    "\n",
    "class LanceDBActor:\n",
    "    def __init__(self, vector_parquet_path, pyarrow_schema):\n",
    "        \n",
    "        # print(os.listdir(vector_parquet_path))\n",
    "        # TODO: consider parameterizing\n",
    "        num_partitions = 1\n",
    "        num_sub_vectors = 5\n",
    "        # assuming 48 core CPU...\n",
    "        os.environ[\"LANCE_CPU_THREADS\"] = \"48\"\n",
    "        os.environ[\"LANCE_IO_THREADS\"] = \"48\"\n",
    "        \n",
    "        # Create local directory, if it does not exist, and connect LanceDB to it.\n",
    "        lance_db_uri = \"/tmp/lancedb\"\n",
    "        try: \n",
    "            os.makedirs(lance_db_uri)\n",
    "        except:\n",
    "            print('lanceDB directory already exists')\n",
    "        self.db = lancedb.connect(lance_db_uri)\n",
    "\n",
    "        # Open Lance table if exists, otherwise create.\n",
    "        try: \n",
    "            self.table_arrow = self.db.open_table(lance_table_name)\n",
    "            print(f'Found LanceDB table {lance_table_name}. Using this for vector search.')\n",
    "        except:\n",
    "            print(f'No LanceDB table {lance_table_name}. Rebuilding from scratch')\n",
    "            self.table_arrow = self.db.create_table(lance_table_name,\n",
    "                                        data=LanceDBActor._get_batches_from_parquet(vector_parquet_path,\n",
    "                                                                                    pyarrow_schema,\n",
    "                                                                                    batch_size=200_000),\n",
    "                                        mode='overwrite'\n",
    "                                        )\n",
    "            self.table_arrow.create_index(\n",
    "                    metric=\"l2\",\n",
    "                    vector_column_name=\"list_col\",\n",
    "                    num_partitions=num_partitions,\n",
    "                    num_sub_vectors=num_sub_vectors\n",
    "                )\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def _get_batches_from_parquet(vector_parquet_path,\n",
    "                                                schema,\n",
    "                                                 batch_size: int = 4096):\n",
    "            \"\"\"\n",
    "            Reads a Parquet file in chunks and yields PyArrow RecordBatches,\n",
    "            displaying progress using tqdm.\n",
    "            \"\"\"\n",
    "            dataset = ds.dataset(vector_parquet_path, format=\"parquet\", schema=schema)\n",
    "\n",
    "            total_rows = dataset.count_rows()\n",
    "            total_batches = np.ceil(total_rows / batch_size)\n",
    "            scanner = dataset.scanner(batch_size=batch_size)\n",
    "\n",
    "            # Wrap the scanner.to_batches() with tqdm\n",
    "            # We use `total_batches` for tqdm's 'total' argument.\n",
    "            with tqdm(total=total_batches, unit=\"batch\", desc=\"Ingesting Parquet Batches\") as pbar:\n",
    "                for batch in scanner.to_batches():\n",
    "                    yield batch\n",
    "                    pbar.update(1) # Manually update progress for each yielded batch\n",
    "                    pbar.set_postfix({\"rows_in_batch\": len(batch)})\n",
    "\n",
    "    def __call__(self, batch: np.ndarray):\n",
    "        results = self.table_arrow.search(batch['item']).limit(1).to_pandas()\n",
    "\n",
    "        return results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f39692ca-47c0-4108-84a0-3b91a05f73e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "num_test_vectors = 1000\n",
    "\n",
    "def create_arrays(n, dimensions):\n",
    "    return [np.random.randint(0, 256, size=dimensions).astype(np.float16) for _ in range(n)]\n",
    "\n",
    "large_query_batch =  ray.data.from_items(create_arrays(num_test_vectors, dimensions=35))\n",
    "output = (large_query_batch.map_batches(LanceDBActor,\n",
    "                              fn_constructor_args=(audio_parquet_path, pyarrow_schema),\n",
    "                              concurrency=3, \n",
    "                              num_cpus=64,\n",
    "                              batch_size=250))\n",
    "external_batch = output.take_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c727536a-d8d8-40a7-9f07-248ad6c7a565",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Make Ray Data dataset and inference with it\n",
    "large_query_batch =  ray.data.from_items(create_arrays(num_test_vectors, dimensions=35))\n",
    "output = (large_query_batch.map_batches(LanceDBActor,\n",
    "                              fn_constructor_args=(audio_parquet_path,pyarrow_schema),\n",
    "                              concurrency=3, \n",
    "                              num_cpus=64,\n",
    "                              batch_size=250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d90d5690-8d31-4f61-ba9f-25b73c39080a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "02a_query_ray_cluster",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
