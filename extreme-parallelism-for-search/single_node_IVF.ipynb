{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -3369811628477256,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a8a213b-8084-4403-8274-f0de57d5de4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Create a single node vector database \n",
    "\n",
    "TODO:\n",
    "- ~~pandas df --> lance DB~~\n",
    "- ~~spark df --> pyarrow --> lance DB~~\n",
    "- ~~batch load in arrow ds and write to lance db~~\n",
    "- [ ] figure out \"hyperparam\" tuning of sub vectors + partitions for IVF index\n",
    "- [ ] Lance x Ray https://lancedb.github.io/lance/integrations/ray.html\n",
    "  - Consider using Ray to write to Lance? \n",
    "\n",
    "Goal is 10M QPM @ $150k (i.e. $15/thousand queries)\n",
    "\n",
    "Current is 24 DBU/H for 5 QPM .4 DBU/M\n",
    "\n",
    "10M/5 = 2M nodes running in parallel to hit 10M QPM\n",
    "\n",
    "2M * 24DBU/H * 1/60 = 800k DBU * .55/DBU = 440k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4da49683-ab2d-4734-8b33-bcb1c5a26ae7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install lancedb numpy  \n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8b6c400-4287-4e41-afa0-12a76bab56bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"catalog\", \"jon_cheung\", \"catalog\")\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "\n",
    "dbutils.widgets.text(\"schema\", \"vizio_poc\", \"schema\")\n",
    "schema = dbutils.widgets.get(\"schema\")\n",
    "\n",
    "dbutils.widgets.text(\"lance_table_name\", \"vizio_poc\", \"lance_table_name\")\n",
    "lance_table_name = dbutils.widgets.get(\"lance_table_name\")\n",
    "\n",
    "dbutils.widgets.text(\"num_vectors\", \"1_000_000_000\", \"num_vectors\")\n",
    "num_vectors = int(dbutils.widgets.get(\"num_vectors\"))\n",
    "\n",
    "dbutils.widgets.text(\"num_test_vectors\", \"100\", \"num_test_vectors\")\n",
    "num_test_vectors = int(dbutils.widgets.get(\"num_test_vectors\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10dbdd7a-f038-40ec-97af-e93941b42e99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Parameters\n",
    "# num_vectors = 1_000_000_000 ## number of vectors to build LanceDB from\n",
    "# catalog = 'jon_cheung'\n",
    "# schema = 'vizio_poc'\n",
    "# n_test_vectors = 100 \n",
    "\n",
    "\n",
    "lance_db_uri = \"/tmp/lancedb\"\n",
    "audio_parquet_path = f'/Volumes/{catalog}/{schema}/{lance_table_name}'\n",
    "# https://lancedb.github.io/lancedb/ann_indexes/#how-to-choose-num_partitions-and-num_sub_vectors-for-ivf_pq-index\n",
    "num_partitions = 25\n",
    "# rows_per_partition = 500_000 ## IVF index parameter; While a very high num_partitions makes individual partition searches faster, there's a point of diminishing returns where the overhead of managing too many small partitions or having to search more partitions (via nprobes) can negate the benefit. However, compared to a low number of partitions (which would lead to large, slow-to-scan partitions), a higher num_partitions is generally better for maximizing throughput.\n",
    "num_sub_vectors = 5 ## IVF index parameter; The number should be a factor of the vector dimension. Because PQ is a lossy compression of the original vector, a higher num_sub_vectors usually results in less space distortion, and thus yields better accuracy.\n",
    "\n",
    "print(audio_parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e10c7c7b-f591-40ca-9c13-fa436de4c504",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE VOLUME IF NOT EXISTS ${catalog}.${schema}.${lance_table_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -3369811628477256,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "653b4fbf-a934-4678-8325-d251e5582e59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Preprocess Spark Dataframe for Lance DB\n",
    "#### Spark dataframe --> Parquet --> PyArrow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fc59f30-7000-4329-950f-e1fd611259b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write samples to parquet if the directory is empty so we can write it to LanceDB via PyArrow\n",
    "import os\n",
    "\n",
    "if not os.path.exists(audio_parquet_path) or not os.listdir(audio_parquet_path):\n",
    "    sdf = spark.read.table('jonathan_mcfadden.vizio_poc.audio_test').limit(num_vectors).select(\"id\", \"list_col\")\n",
    "    sdf.write.mode('overwrite').parquet(audio_parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "319c55a2-c08b-4438-b63e-1d84a46636e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load in Parquet as PyArrow Dataset and modify schema for LanceDB\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "\n",
    "# Define a custom PyArrow schema\n",
    "pyarrow_schema = pa.schema(\n",
    "    [\n",
    "        pa.field(\"id\", pa.int64()),\n",
    "        pa.field(\"list_col\", pa.list_(pa.float16(), 35)),   # Fixed size list\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46c88266-d1a2-4eae-aba0-0d3a1afc463a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_batches_from_parquet(parquet_path: str, schema: pa.Schema, batch_size: int = 1024):\n",
    "    \"\"\"\n",
    "    Reads a Parquet file in chunks and yields PyArrow RecordBatches.\n",
    "    \"\"\"\n",
    "    dataset = ds.dataset(parquet_path, format=\"parquet\", schema=schema)\n",
    "    scanner = dataset.scanner(batch_size=batch_size) # Specify batch_size for iteration\n",
    "    for batch in scanner.to_batches():\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f54a070-93b4-4a35-be60-cc1fde0c733a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm # Use tqdm.auto for intelligent display (console/notebook)\n",
    "\n",
    "def get_batches_from_parquet_with_progress(parquet_path: str, schema: pa.Schema, batch_size: int = 4096):\n",
    "    \"\"\"\n",
    "    Reads a Parquet file in chunks and yields PyArrow RecordBatches,\n",
    "    displaying progress using tqdm.\n",
    "    \"\"\"\n",
    "    dataset = ds.dataset(parquet_path, format=\"parquet\", schema=schema)\n",
    "\n",
    "    # Estimate total number of rows for tqdm.\n",
    "    # Note: dataset.count_rows() can be slow for very large datasets if metadata isn't optimized.\n",
    "    # If performance is an issue here, you might need to pre-calculate or use an estimate.\n",
    "    try:\n",
    "        total_rows = dataset.count_rows()\n",
    "        total_batches = np.ceil(total_rows / batch_size)\n",
    "    except Exception:\n",
    "        # Fallback if count_rows fails or is too slow.\n",
    "        # tqdm will then run without a fixed total, just showing counts.\n",
    "        total_rows = None\n",
    "        total_batches = None\n",
    "        print(\"Warning: Could not determine total rows for precise tqdm progress. Progress will be based on batches.\")\n",
    "\n",
    "    scanner = dataset.scanner(batch_size=batch_size)\n",
    "\n",
    "    # Wrap the scanner.to_batches() with tqdm\n",
    "    # We use `total_batches` for tqdm's 'total' argument.\n",
    "    with tqdm(total=total_batches, unit=\"batch\", desc=\"Ingesting Parquet Batches\") as pbar:\n",
    "        for batch in scanner.to_batches():\n",
    "            yield batch\n",
    "            pbar.update(1) # Manually update progress for each yielded batch\n",
    "            pbar.set_postfix({\"rows_in_batch\": len(batch)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdc317b5-2ec9-4548-a150-5ae081633529",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Create LanceDB and index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e164579-4e30-4062-be46-dbe3a3989eec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "mkdir -p /tmp/lancedb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92a9a0ec-a2c8-4ec4-a360-21624a06567b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lancedb\n",
    "\n",
    "\n",
    "db = lancedb.connect(lance_db_uri)\n",
    "\n",
    "if lance_table_name in db.table_names():\n",
    "    db.drop_table(lance_table_name)\n",
    "    print(f\"Dropped existing table: {lance_table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ea1b1cb-e71e-482e-a4b3-3ff61dd3d0f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_arrow = db.create_table(\n",
    "    lance_table_name,\n",
    "    data=get_batches_from_parquet_with_progress(audio_parquet_path, pyarrow_schema, batch_size=200_000),\n",
    "    mode=\"overwrite\"\n",
    ")\n",
    "\n",
    "\n",
    "table_arrow.create_index(\n",
    "        metric=\"l2\",\n",
    "        vector_column_name=\"list_col\",\n",
    "        num_partitions=20,\n",
    "        num_sub_vectors=num_sub_vectors\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40cbd9d9-91bf-446a-8197-40f8c871157e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# LanceDB Cloud runs distributed indexing with Ray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f59e100-5852-4d0b-8280-4372905e0c0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Test inference \n",
    "3.1 Single-thread inference  \n",
    "3.2 Thread pool inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49eb8614-0503-4131-98ac-de9a40a8ae8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "os.environ[\"LANCE_CPU_THREADS\"] = \"96\"\n",
    "os.environ[\"LANCE_IO_THREADS\"] = \"96\"\n",
    "\n",
    "def create_arrays(n, dimensions):\n",
    "    return [np.random.randint(0, 256, size=dimensions).astype(np.float16) for _ in range(n)]\n",
    "\n",
    "def search_database(table, vector, limit):\n",
    "    \"\"\"\n",
    "    Helper function to perform a single LanceDB search query.\n",
    "    This function will be executed by each thread in the thread pool.\n",
    "    \"\"\"\n",
    "    local_db = lancedb.connect(lance_db_uri) # Connect in each process\n",
    "    table = local_db.open_table(lance_table_name)\n",
    "\n",
    "    return table.search(vector).limit(limit).to_pandas()\n",
    "\n",
    "test_micro_batch = create_arrays(n_test_vectors, dimensions=35)\n",
    "\n",
    "start = time.time()\n",
    "search_database(lance_table_name, \n",
    "                test_micro_batch, \n",
    "                limit=1)\n",
    "run_time = time.time() - start\n",
    "print(f\"Batch search completed in {run_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52c936bc-fd5a-46a2-ad18-1bc75bb9f5fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import random\n",
    "\n",
    "# num_vectors = 1_000_000_000\n",
    "  \n",
    "# def create_arrays(n, dimensions):\n",
    "#     return [np.random.randint(0, 256, size=dimensions).astype(np.float16) for _ in range(n)]\n",
    "  \n",
    "\n",
    "# data = pd.DataFrame(\n",
    "#     {\n",
    "#         \"vector\": create_arrays(num_vectors, dimensions=35),\n",
    "#         \"id\": np.arange(num_vectors),\n",
    "#     }\n",
    "# )\n",
    "# tbl = db.create_table(\"my_table_pandas_100m\", data, mode=\"overwrite\")\n",
    "# # Create IVF index on top of table to improve latency but decrease accuracy\n",
    "# # https://lancedb.github.io/lancedb/ann_indexes/#how-to-choose-num_partitions-and-num_sub_vectors-for-ivf_pq-index\n",
    "# # - num_sub_vectors = The number should be a factor of the vector dimension. Because PQ is a lossy compression of the original vector, a higher num_sub_vectors usually results in less space distortion, and thus yields better accuracy.\n",
    "# # - num_partitions = While a very high num_partitions makes individual partition searches faster, there's a point of diminishing returns where the overhead of managing too many small partitions or having to search more partitions (via nprobes) can negate the benefit. However, compared to a low number of partitions (which would lead to large, slow-to-scan partitions), a higher num_partitions is generally better for maximizing throughput.\n",
    "# tbl.create_index(metric=\"l2\", num_partitions=1000, num_sub_vectors=5)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3369811628477265,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "single_node_IVF",
   "widgets": {
    "catalog": {
     "currentValue": "jon_cheung",
     "nuid": "25dedb42-ea0b-4568-86cc-1bbb22fd56c5",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "jon_cheung",
      "label": "catalog",
      "name": "catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "jon_cheung",
      "label": "catalog",
      "name": "catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "lance_table_name": {
     "currentValue": "from_spark_1B",
     "nuid": "85f61345-490a-42eb-b79a-177ec8c06ce0",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "vizio_poc",
      "label": "lance_table_name",
      "name": "lance_table_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "vizio_poc",
      "label": "lance_table_name",
      "name": "lance_table_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "num_test_vectors": {
     "currentValue": "100",
     "nuid": "b97b401d-793b-494e-8a25-7a48dd570b6c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "100",
      "label": "num_test_vectors",
      "name": "num_test_vectors",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "100",
      "label": "num_test_vectors",
      "name": "num_test_vectors",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "num_vectors": {
     "currentValue": "1_000_000_000",
     "nuid": "855610fa-44a8-42a4-9423-1a8f0ffeb4d5",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "1_000_000_000",
      "label": "num_vectors",
      "name": "num_vectors",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "1_000_000_000",
      "label": "num_vectors",
      "name": "num_vectors",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "schema": {
     "currentValue": "vizio_poc",
     "nuid": "a126646a-2b76-4b1f-891c-0bb88714608f",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "vizio_poc",
      "label": "schema",
      "name": "schema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "vizio_poc",
      "label": "schema",
      "name": "schema",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
