{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -3369811628454541,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a8a213b-8084-4403-8274-f0de57d5de4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Create a single node vector database \n",
    "\n",
    "TODO:\n",
    "- [X] pandas df --> lance DB \n",
    "- [X] spark df --> pyarrow --> lance DB \n",
    "- [ ] batch load in arrow ds and write to lance db (almost done)\n",
    "- [ ] figure out how to \"hyperparam\" tuning of sub vectors + partitions for IVF index\n",
    "\n",
    "\n",
    "Goal is 10M QPM @ $15k (i.e. $15/thousand queries)\n",
    "\n",
    "Current is 24 DBU/H for 100 QPM .4 DBU/M\n",
    "\n",
    "10M/100 = 100k nodes running in parallel to hit 10M QPM\n",
    "\n",
    "10k * 24DBU/H * 1/60 = 4000DBU * .55/DBU = 22k\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -3369811628454541,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4da49683-ab2d-4734-8b33-bcb1c5a26ae7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install lancedb numpy  \n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -3369811628454541,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10dbdd7a-f038-40ec-97af-e93941b42e99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Parameters\n",
    "num_vectors = 1_000_000_000 ## number of vectors to build LanceDB from\n",
    "lance_db_uri = \"/tmp/lancedb\"\n",
    "lance_table_name = \"from_spark_100M\"\n",
    "audio_parquet_path = '/Volumes/jon_cheung/vizio_poc/audio_parquets'\n",
    "# https://lancedb.github.io/lancedb/ann_indexes/#how-to-choose-num_partitions-and-num_sub_vectors-for-ivf_pq-index\n",
    "rows_per_partition = 500_000 ## IVF index parameter; The number should be a factor of the vector dimension. Because PQ is a lossy compression of the original vector, a higher num_sub_vectors usually results in less space distortion, and thus yields better accuracy.\n",
    "num_sub_vectors = 5 ## IVF index parameter; While a very high num_partitions makes individual partition searches faster, there's a point of diminishing returns where the overhead of managing too many small partitions or having to search more partitions (via nprobes) can negate the benefit. However, compared to a low number of partitions (which would lead to large, slow-to-scan partitions), a higher num_partitions is generally better for maximizing throughput.\n",
    "n_test_vectors = 100 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -3369811628454541,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "653b4fbf-a934-4678-8325-d251e5582e59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Preprocess Spark Dataframe for Lance DB\n",
    "#### Spark dataframe --> Parquet --> PyArrow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -3369811628454541,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fc59f30-7000-4329-950f-e1fd611259b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write samples to parquet so we can write it to LanceDB via PyArrow\n",
    "sdf = spark.read.table('jonathan_mcfadden.vizio_poc.audio_test').limit(num_vectors).select(\"id\", \"list_col\")\n",
    "\n",
    "sdf.write.mode('overwrite').parquet(audio_parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -3369811628454541,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "319c55a2-c08b-4438-b63e-1d84a46636e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load in Parquet as PyArrow Dataset and modify schema for LanceDB\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "\n",
    "# Define a custom PyArrow schema\n",
    "pyarrow_schema = pa.schema(\n",
    "    [\n",
    "        pa.field(\"id\", pa.int64()),\n",
    "        pa.field(\"list_col\", pa.list_(pa.float16(), 35)),   # Fixed size list\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -3369811628454541,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46c88266-d1a2-4eae-aba0-0d3a1afc463a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_batches_from_parquet(parquet_path: str, schema: pa.Schema, batch_size: int = 1024):\n",
    "    \"\"\"\n",
    "    Reads a Parquet file in chunks and yields PyArrow RecordBatches.\n",
    "    \"\"\"\n",
    "    dataset = ds.dataset(parquet_path, format=\"parquet\", schema=schema)\n",
    "    scanner = dataset.scanner(batch_size=batch_size) # Specify batch_size for iteration\n",
    "    for batch in scanner.to_batches():\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -3369811628454541,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f54a070-93b4-4a35-be60-cc1fde0c733a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm # Use tqdm.auto for intelligent display (console/notebook)\n",
    "\n",
    "def get_batches_from_parquet_with_progress(parquet_path: str, schema: pa.Schema, batch_size: int = 4096):\n",
    "    \"\"\"\n",
    "    Reads a Parquet file in chunks and yields PyArrow RecordBatches,\n",
    "    displaying progress using tqdm.\n",
    "    \"\"\"\n",
    "    dataset = ds.dataset(parquet_path, format=\"parquet\", schema=schema)\n",
    "\n",
    "    # Estimate total number of rows for tqdm.\n",
    "    # Note: dataset.count_rows() can be slow for very large datasets if metadata isn't optimized.\n",
    "    # If performance is an issue here, you might need to pre-calculate or use an estimate.\n",
    "    try:\n",
    "        total_rows = dataset.count_rows()\n",
    "        total_batches = np.ceil(total_rows / batch_size)\n",
    "    except Exception:\n",
    "        # Fallback if count_rows fails or is too slow.\n",
    "        # tqdm will then run without a fixed total, just showing counts.\n",
    "        total_rows = None\n",
    "        total_batches = None\n",
    "        print(\"Warning: Could not determine total rows for precise tqdm progress. Progress will be based on batches.\")\n",
    "\n",
    "    scanner = dataset.scanner(batch_size=batch_size)\n",
    "\n",
    "    # Wrap the scanner.to_batches() with tqdm\n",
    "    # We use `total_batches` for tqdm's 'total' argument.\n",
    "    with tqdm(total=total_batches, unit=\"batch\", desc=\"Ingesting Parquet Batches\") as pbar:\n",
    "        for batch in scanner.to_batches():\n",
    "            yield batch\n",
    "            pbar.update(1) # Manually update progress for each yielded batch\n",
    "            pbar.set_postfix({\"rows_in_batch\": len(batch)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -3369811628454541,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdc317b5-2ec9-4548-a150-5ae081633529",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Create LanceDB and index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -3369811628454541,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e164579-4e30-4062-be46-dbe3a3989eec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "mkdir -p /tmp/lancedb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -3369811628454541,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92a9a0ec-a2c8-4ec4-a360-21624a06567b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lancedb\n",
    "\n",
    "\n",
    "db = lancedb.connect(lance_db_uri)\n",
    "\n",
    "if lance_table_name in db.table_names():\n",
    "    db.drop_table(lance_table_name)\n",
    "    print(f\"Dropped existing table: {lance_table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -3369811628454541,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ea1b1cb-e71e-482e-a4b3-3ff61dd3d0f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_arrow = db.create_table(\n",
    "    lance_table_name,\n",
    "    data=get_batches_from_parquet_with_progress(audio_parquet_path, pyarrow_schema, batch_size=100_000),\n",
    "    mode=\"overwrite\"\n",
    ")\n",
    "\n",
    "table_arrow.create_index(metric=\"l2\", \n",
    "                         vector_column_name=\"list_col\",\n",
    "                         num_subgraphs =100,\n",
    "                         num_sub_vectors= num_sub_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -3369811628454541,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40cbd9d9-91bf-446a-8197-40f8c871157e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## SINGLE BATCH\n",
    "# pyarrow_dataset = ds.dataset(audio_parquet_path, format=\"parquet\", schema=pyarrow_schema)\n",
    "# table_arrow = db.create_table(lance_table_name, data=pyarrow_dataset, mode=\"overwrite\")\n",
    "# table_arrow.create_index(metric=\"l2\", vector_column_name=\"list_col\", num_partitions=5, num_sub_vectors=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -3369811628454541,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f59e100-5852-4d0b-8280-4372905e0c0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Test inference \n",
    "3.1 Single-thread inference  \n",
    "3.2 Thread pool inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -3369811628454541,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3874c7c5-f704-40fd-b847-feeb193abf54",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "thread pool search"
    }
   },
   "outputs": [],
   "source": [
    "# --- ThreadPool Search Function ---\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "def create_arrays(n, dimensions):\n",
    "    return [np.random.randint(0, 256, size=dimensions).astype(np.float16) for _ in range(n)]\n",
    "\n",
    "def search_single_vector(table, vector, limit):\n",
    "    \"\"\"\n",
    "    Helper function to perform a single LanceDB search query.\n",
    "    This function will be executed by each thread in the thread pool.\n",
    "    \"\"\"\n",
    "    local_db = lancedb.connect(lance_db_uri) # Connect in each process\n",
    "    table = local_db.open_table(lance_table_name)\n",
    "\n",
    "    return table.search(vector).limit(limit).to_pandas()\n",
    "\n",
    "def threaded_lancedb_search(table, test_vectors, limit=1, max_workers=None):\n",
    "    \"\"\"\n",
    "    Performs LanceDB searches for a list of test vectors using a ThreadPoolExecutor.\n",
    "\n",
    "    Args:\n",
    "        table: The LanceDB table object to search against.\n",
    "        test_vectors: A list of vectors (e.g., NumPy arrays) to query.\n",
    "        limit: The maximum number of results to return for each query.\n",
    "        max_workers: The maximum number of threads to use. If None, it defaults\n",
    "                     to the number of processors on the machine, multiplied by 5,\n",
    "                     if the number of processors is less than 5. Otherwise, it\n",
    "                     defaults to the number of processors plus 4.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame containing the combined results from all searches.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    # Using ThreadPoolExecutor for concurrent execution\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit each search query to the executor\n",
    "        # map() applies the search_single_vector function to each vector in test_vectors\n",
    "        # The 'table' and 'limit' arguments are fixed for all calls.\n",
    "        futures = executor.map(lambda vec: search_single_vector(table, vec, limit), test_vectors)\n",
    "\n",
    "        # Collect results as they complete\n",
    "        for i, res_df in enumerate(futures):\n",
    "            all_results.append(res_df)\n",
    "            # print(f\"Query {i+1}/{len(test_vectors)} completed.\") # Optional: progress indicator\n",
    "\n",
    "    # Concatenate all individual DataFrames into one large DataFrame\n",
    "    if all_results:\n",
    "        return pd.concat(all_results, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame() # Return empty DataFrame if no results\n",
    "\n",
    "test_micro_batch = create_arrays(n_test_vectors, 35)\n",
    "\n",
    "start = time.time()\n",
    "print(f\"\\nStarting threaded search for {n_test_vectors} queries...\")\n",
    "\n",
    "# Perform the threaded search\n",
    "# You can adjust max_workers based on your system's capabilities and workload\n",
    "results_threaded = threaded_lancedb_search(lance_table_name, test_micro_batch, limit=1, max_workers=96) \n",
    "run_time = time.time() - start\n",
    "print(f\"Threaded search completed in {run_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2232de0a-b25d-451a-ba3e-709404619eb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52c936bc-fd5a-46a2-ad18-1bc75bb9f5fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "num_vectors = 1_000_000_000\n",
    "  \n",
    "def create_arrays(n, dimensions):\n",
    "    return [np.random.randint(0, 256, size=dimensions).astype(np.float16) for _ in range(n)]\n",
    "  \n",
    "\n",
    "data = pd.DataFrame(\n",
    "    {\n",
    "        \"vector\": create_arrays(num_vectors, dimensions=35),\n",
    "        \"id\": np.arange(num_vectors),\n",
    "    }\n",
    ")\n",
    "tbl = db.create_table(\"my_table_pandas_100m\", data, mode=\"overwrite\")\n",
    "# Create IVF index on top of table to improve latency but decrease accuracy\n",
    "# https://lancedb.github.io/lancedb/ann_indexes/#how-to-choose-num_partitions-and-num_sub_vectors-for-ivf_pq-index\n",
    "# - num_sub_vectors = The number should be a factor of the vector dimension. Because PQ is a lossy compression of the original vector, a higher num_sub_vectors usually results in less space distortion, and thus yields better accuracy.\n",
    "# - num_partitions = While a very high num_partitions makes individual partition searches faster, there's a point of diminishing returns where the overhead of managing too many small partitions or having to search more partitions (via nprobes) can negate the benefit. However, compared to a low number of partitions (which would lead to large, slow-to-scan partitions), a higher num_partitions is generally better for maximizing throughput.\n",
    "tbl.create_index(metric=\"l2\", num_partitions=1000, num_sub_vectors=5)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3369811628438790,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "single_node_IVF",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
