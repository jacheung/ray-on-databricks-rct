{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4da49683-ab2d-4734-8b33-bcb1c5a26ae7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install lancedb numpy\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2928ff2-637e-467f-955a-0f326e70888e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lancedb\n",
    "\n",
    "def create_arrays(n, dimensions):\n",
    "    return [np.random.randint(0, 256, size=dimensions).astype(np.float16) for _ in range(n)]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e164579-4e30-4062-be46-dbe3a3989eec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "mkdir -p /tmp/lancedb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92a9a0ec-a2c8-4ec4-a360-21624a06567b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "uri = \"/tmp/lancedb\"\n",
    "db = lancedb.connect(uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52c936bc-fd5a-46a2-ad18-1bc75bb9f5fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "num_vectors = 1_000_000_000\n",
    "  \n",
    "def create_arrays(n, dimensions):\n",
    "    return [np.random.randint(0, 256, size=dimensions).astype(np.float16) for _ in range(n)]\n",
    "  \n",
    "\n",
    "data = pd.DataFrame(\n",
    "    {\n",
    "        \"vector\": create_arrays(num_vectors, dimensions=35),\n",
    "        \"id\": np.arange(num_vectors),\n",
    "    }\n",
    ")\n",
    "tbl = db.create_table(\"my_table_pandas_100m\", data, mode=\"overwrite\")\n",
    "# Create IVF index on top of table to improve latency but decrease accuracy\n",
    "# https://lancedb.github.io/lancedb/ann_indexes/#how-to-choose-num_partitions-and-num_sub_vectors-for-ivf_pq-index\n",
    "# - num_sub_vectors = The number should be a factor of the vector dimension. Because PQ is a lossy compression of the original vector, a higher num_sub_vectors usually results in less space distortion, and thus yields better accuracy.\n",
    "# - num_partitions = On SIFT-1M dataset, our benchmark shows that keeping each partition 4K-8K rows lead to a good latency / recall.\n",
    "tbl.create_index(metric=\"l2\", num_partitions=1000, num_sub_vectors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c7fb6f6-e64c-423b-9157-98ab3f190b6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # 20s for brute force search with 100 queries and returning top 5\n",
    "# # 2s for IVF search with 100 queries\n",
    "# # 15s for IVF search w/ 1k queries\n",
    "# n_test_vectors = 1000\n",
    "# test_micro_batch = create_arrays(n_test_vectors, 35)\n",
    "# results = tbl.search(test_micro_batch).limit(5).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e2c121e-757c-4202-96a8-fe581b9041c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "n_test_vectors = 1000\n",
    "test_micro_batch = create_arrays(n_test_vectors, 35)\n",
    "\n",
    "def search_query(query_vector):\n",
    "    \"\"\"Function to be executed in parallel for each query vector.\"\"\"\n",
    "    # Re-establish connection or ensure thread-safety if db connection is not picklable\n",
    "    # For local LanceDB, you might need to re-connect in each process if the connection object isn't serializable\n",
    "    # or if the underlying Rust objects are not designed for cross-process sharing.\n",
    "    # Often, opening a new connection per process/thread is safer.\n",
    "    local_db = lancedb.connect(uri) # Connect in each process\n",
    "    local_tbl = local_db.open_table(\"my_table_pandas_100m\")\n",
    "    \n",
    "    return local_tbl.search(query_vector).limit(5).to_pandas()\n",
    "\n",
    "search_query(test_micro_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91c376e3-732b-4564-bfde-a5b17474780a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- ThreadPool Search Function ---\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "\n",
    "def search_single_vector(table, vector, limit):\n",
    "    \"\"\"\n",
    "    Helper function to perform a single LanceDB search query.\n",
    "    This function will be executed by each thread in the thread pool.\n",
    "    \"\"\"\n",
    "    return table.search(vector).limit(limit).to_pandas()\n",
    "\n",
    "def threaded_lancedb_search(table, test_vectors, limit=1, max_workers=None):\n",
    "    \"\"\"\n",
    "    Performs LanceDB searches for a list of test vectors using a ThreadPoolExecutor.\n",
    "\n",
    "    Args:\n",
    "        table: The LanceDB table object to search against.\n",
    "        test_vectors: A list of vectors (e.g., NumPy arrays) to query.\n",
    "        limit: The maximum number of results to return for each query.\n",
    "        max_workers: The maximum number of threads to use. If None, it defaults\n",
    "                     to the number of processors on the machine, multiplied by 5,\n",
    "                     if the number of processors is less than 5. Otherwise, it\n",
    "                     defaults to the number of processors plus 4.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame containing the combined results from all searches.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    # Using ThreadPoolExecutor for concurrent execution\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit each search query to the executor\n",
    "        # map() applies the search_single_vector function to each vector in test_vectors\n",
    "        # The 'table' and 'limit' arguments are fixed for all calls.\n",
    "        futures = executor.map(lambda vec: search_single_vector(table, vec, limit), test_vectors)\n",
    "\n",
    "        # Collect results as they complete\n",
    "        for i, res_df in enumerate(futures):\n",
    "            all_results.append(res_df)\n",
    "            # print(f\"Query {i+1}/{len(test_vectors)} completed.\") # Optional: progress indicator\n",
    "\n",
    "    # Concatenate all individual DataFrames into one large DataFrame\n",
    "    if all_results:\n",
    "        return pd.concat(all_results, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame() # Return empty DataFrame if no results\n",
    "\n",
    "# --- Example Usage ---\n",
    "n_test_vectors = 10_000\n",
    "test_micro_batch = create_arrays(n_test_vectors, 35)\n",
    "\n",
    "start = time.time()\n",
    "print(f\"\\nStarting threaded search for {n_test_vectors} queries...\")\n",
    "\n",
    "# Perform the threaded search\n",
    "# You can adjust max_workers based on your system's capabilities and workload\n",
    "results_threaded = threaded_lancedb_search(tbl, test_micro_batch, limit=5, max_workers=64) \n",
    "run_time = time.time() - start\n",
    "print(f\"Threaded search completed in {run_time:.2f} seconds.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2266247120185500,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "single_node_IVF",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
