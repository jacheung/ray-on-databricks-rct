{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "680619e3-f4bd-453d-9142-b5aeef018604",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install ray[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dad59dca-6b22-41be-8057-f11dd5b469d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ray.init(num_cpus=4) # Assuming at least 4 CPUs for this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f994821-861b-401f-9010-eabcc6fa44f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "@ray.remote\n",
    "class ShardActor:\n",
    "    def __init__(self, shard_id, data_path):\n",
    "        self.shard_id = shard_id\n",
    "        # Load shard data here\n",
    "        print(f\"ShardActor {shard_id} initialized on {ray.util.get_node_ip_address()}\")\n",
    "        self.data = f\"Data for shard {shard_id}\" # Placeholder\n",
    "        # For vector search, load your HNSW index etc.\n",
    "\n",
    "    def lookup(self, query):\n",
    "        # Perform lookup on self.data\n",
    "        # print(f\"Lookup on shard {self.shard_id} for query {query}\")\n",
    "        return f\"Result from shard {self.shard_id} for {query}\"\n",
    "\n",
    "# Instead of: shard_actors = {i: ShardActor.remote(i, f\"path_to_shard_{i}\") for i in range(num_shards)}\n",
    "\n",
    "# Use multiple actors for each shard (especially for anticipated hot shards)\n",
    "num_shards = 4\n",
    "num_actors_per_shard = 2 # Or more for hot shards, less for cold\n",
    "\n",
    "shard_actor_groups = {}\n",
    "for i in range(num_shards):\n",
    "    shard_actor_groups[i] = [ShardActor.remote(i, f\"path_to_shard_{i}\") for _ in range(num_actors_per_shard)]\n",
    "\n",
    "@ray.remote\n",
    "class Router:\n",
    "    def __init__(self, shard_actor_groups):\n",
    "        self.shard_actor_groups = shard_actor_groups\n",
    "        self.counters = {shard_id: 0 for shard_id in shard_actor_groups.keys()}\n",
    "\n",
    "    def route_request(self, query):\n",
    "        shard_id = self._determine_shard(query) # Implement your sharding logic\n",
    "\n",
    "        # Round-robin or intelligent load balancing among actors for the same shard\n",
    "        actor_idx = self.counters[shard_id] % len(self.shard_actor_groups[shard_id])\n",
    "        self.counters[shard_id] += 1\n",
    "\n",
    "        selected_actor = self.shard_actor_groups[shard_id][actor_idx]\n",
    "        return ray.get(selected_actor.lookup.remote(query))\n",
    "\n",
    "    def _determine_shard(self, query):\n",
    "        # Simple example: Assuming query contains a shard_id hint\n",
    "        # In real scenario, this would be based on vector content or ID\n",
    "        if \"shard_1\" in query:\n",
    "            return 1\n",
    "        elif \"shard_0\" in query:\n",
    "            return 0\n",
    "        elif \"shard_2\" in query:\n",
    "            return 2\n",
    "        else: # Default for demonstration\n",
    "            return 3\n",
    "\n",
    "\n",
    "# --- Demo ---\n",
    "router = Router.remote(shard_actor_groups)\n",
    "\n",
    "# Simulate skewed load towards shard 1\n",
    "print(\"\\nSimulating skewed load...\")\n",
    "results = []\n",
    "for _ in range(10): # More requests for shard 1\n",
    "    results.append(router.route_request.remote(\"query for shard_1\"))\n",
    "for _ in range(2): # Fewer requests for other shards\n",
    "    results.append(router.route_request.remote(\"query for shard_0\"))\n",
    "    results.append(router.route_request.remote(\"query for shard_2\"))\n",
    "    results.append(router.route_request.remote(\"query for shard_3\"))\n",
    "\n",
    "ray.get(results)\n",
    "print(\"All lookups complete.\")\n",
    "\n",
    "# You would observe that requests for shard 1 are distributed among the multiple actors dedicated to shard 1.\n",
    "\n",
    "# ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3d515f1-5a25-4890-bdf9-6adda28d8ace",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ray.get(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b40df319-44dc-4c6b-91d9-ae017edd75fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "ray_vector_search_with_router",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
