{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c350411-649e-44e4-92e2-b5fc3f20bb74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install ray[all] lancedb\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29a0d83e-12f0-4fdf-8d95-328cfbaab549",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Shard plan\n",
    "1. Create parquet shards (e.g., 10) --> See 00a_shard_data_preparation\n",
    "2. Construct inferencing Ray actors\n",
    "  * Ray actor, as input, receive shard num.\n",
    "  * Ray actor build local lance DB instance using shard from parquet\n",
    "3. inference:\n",
    "  * ray.data a batch (e.g. 1k)\n",
    "  * rerank?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5037d8bf-6c60-4167-863e-5f534f5f76ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "catalog_name = 'jon_cheung'\n",
    "schema_name = 'vizio_poc'\n",
    "lance_table_name = 'audio_100M_chunk_shard_5'\n",
    "num_test_vectors = 5_000\n",
    "\n",
    "\n",
    "audio_parquet_path = f'/Volumes/{catalog_name}/{schema_name}/{lance_table_name}'\n",
    "pyarrow_schema = pa.schema(\n",
    "    [\n",
    "        pa.field(\"id\", pa.int64()),\n",
    "        # pa.field(\"list_col\", pa.list_(pa.uint8(), 35)),   # Fixed size list\n",
    "        pa.field(\"list_col\", pa.list_(pa.float16(), 35)),   # Fixed size list\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1a36a42-2121-4f57-af1b-baabb97c8f51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create fuse volume for Ray to write back to Spark\n",
    "import os\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service import catalog\n",
    "\n",
    "w = WorkspaceClient()\n",
    "UC_fuse_temp_dir = f'/Volumes/{catalog_name}/{schema_name}/ray_data_tmp_dir'\n",
    "if not os.path.exists(UC_fuse_temp_dir):\n",
    "  created_volume = w.volumes.create(catalog_name=catalog_name,\n",
    "                                    schema_name=schema_name,\n",
    "                                    name='ray_data_tmp_dir',\n",
    "                                    volume_type=catalog.VolumeType.MANAGED\n",
    "                                    )\n",
    "\n",
    "os.environ['RAY_UC_VOLUMES_FUSE_TEMP_DIR'] = f'/Volumes/{catalog_name}/{schema_name}/ray_data_tmp_dir'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2577209-7193-40c7-9d33-3b0aeb9ba388",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.util.spark import setup_ray_cluster, shutdown_ray_cluster, setup_global_ray_cluster\n",
    "\n",
    "\n",
    "restart = True\n",
    "if restart is True:\n",
    "  try:\n",
    "    shutdown_ray_cluster()\n",
    "  except:\n",
    "    pass\n",
    "  try:\n",
    "    ray.shutdown()\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "setup_global_ray_cluster(\n",
    "  min_worker_nodes=3,\n",
    "  max_worker_nodes=3,\n",
    "  num_cpus_worker_node=64,\n",
    "  num_gpus_worker_node=0,\n",
    "  collect_log_to_path=\"/dbfs/Users/jon.cheung@databricks.com/ray_collected_logs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4d23297-c1ac-48c8-9cd1-0f9c648d6569",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pyarrow.dataset as ds\n",
    "import lancedb\n",
    "from tqdm import tqdm\n",
    "\n",
    "num_closest_vectors = 1\n",
    "\n",
    "class LanceDBActor:\n",
    "    def __init__(self, vector_parquet_path, pyarrow_schema):\n",
    "        \n",
    "        # print(os.listdir(vector_parquet_path))\n",
    "        # TODO: consider parameterizing\n",
    "        num_partitions = 1\n",
    "        num_sub_vectors = 5\n",
    "        # assuming 48 core CPU...\n",
    "        os.environ[\"LANCE_CPU_THREADS\"] = \"48\"\n",
    "        os.environ[\"LANCE_IO_THREADS\"] = \"48\"\n",
    "        \n",
    "        # Create local directory, if it does not exist, and connect LanceDB to it.\n",
    "        lance_db_uri = \"/tmp/lancedb\"\n",
    "        try: \n",
    "            os.makedirs(lance_db_uri)\n",
    "        except:\n",
    "            print('lanceDB directory already exists')\n",
    "        self.db = lancedb.connect(lance_db_uri)\n",
    "\n",
    "        # Open Lance table if exists, otherwise create.\n",
    "        try: \n",
    "            self.table_arrow = self.db.open_table(lance_table_name)\n",
    "            print(f'Found LanceDB table {lance_table_name}. Using this for vector search.')\n",
    "        except:\n",
    "            print(f'No LanceDB table {lance_table_name}. Rebuilding from scratch')\n",
    "            self.table_arrow = self.db.create_table(lance_table_name,\n",
    "                                        data=LanceDBActor._get_batches_from_parquet(vector_parquet_path,\n",
    "                                                                                    pyarrow_schema,\n",
    "                                                                                    batch_size=200_000),\n",
    "                                        mode='overwrite'\n",
    "                                        )\n",
    "            self.table_arrow.create_index(\n",
    "                    metric=\"l2\",\n",
    "                    vector_column_name=\"list_col\",\n",
    "                    num_partitions=num_partitions,\n",
    "                    num_sub_vectors=num_sub_vectors\n",
    "                )\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def _get_batches_from_parquet(vector_parquet_path,\n",
    "                                                schema,\n",
    "                                                 batch_size: int = 4096):\n",
    "            \"\"\"\n",
    "            Reads a Parquet file in chunks and yields PyArrow RecordBatches,\n",
    "            displaying progress using tqdm.\n",
    "            \"\"\"\n",
    "            dataset = ds.dataset(vector_parquet_path, format=\"parquet\", schema=schema)\n",
    "\n",
    "            total_rows = dataset.count_rows()\n",
    "            total_batches = np.ceil(total_rows / batch_size)\n",
    "            scanner = dataset.scanner(batch_size=batch_size)\n",
    "\n",
    "            # Wrap the scanner.to_batches() with tqdm\n",
    "            # We use `total_batches` for tqdm's 'total' argument.\n",
    "            with tqdm(total=total_batches, unit=\"batch\", desc=\"Ingesting Parquet Batches\") as pbar:\n",
    "                for batch in scanner.to_batches():\n",
    "                    yield batch\n",
    "                    pbar.update(1) # Manually update progress for each yielded batch\n",
    "                    pbar.set_postfix({\"rows_in_batch\": len(batch)})\n",
    "\n",
    "    def __call__(self, batch: np.ndarray):\n",
    "        results = self.table_arrow.search(batch['item']).limit(1).to_pandas()\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "\n",
    "def create_arrays(n, dimensions):\n",
    "    return [np.random.randint(0, 256, size=dimensions).astype(np.float16) for _ in range(n)]\n",
    "\n",
    "# Make Ray Data dataset and inference with it\n",
    "large_query_batch =  ray.data.from_items(create_arrays(num_test_vectors, dimensions=35))\n",
    "output = (large_query_batch.map_batches(LanceDBActor,\n",
    "                              fn_constructor_args=(audio_parquet_path,pyarrow_schema),\n",
    "                              concurrency=3, \n",
    "                              num_cpus=64,\n",
    "                              batch_size=250))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5618a9d-386a-4d87-a2e8-e74423b181eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "all_results = output.take_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aed17867-72c0-4063-b69d-6db559633a5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "num_test_vectors = 10000\n",
    "large_query_batch =  ray.data.from_items(create_arrays(num_test_vectors, dimensions=35))\n",
    "output = (large_query_batch.map_batches(LanceDBActor,\n",
    "                              fn_constructor_args=(audio_parquet_path,pyarrow_schema),\n",
    "                              concurrency=3, \n",
    "                              num_cpus=64,\n",
    "                              batch_size=250))\n",
    "second_batch = output.take_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "406b4024-f8a9-4c7a-b945-f895f0e77b74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "second_batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de43112a-4335-40a3-8f7c-3e1590b9c5e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # # write grouped results to a Delta table\n",
    "# ray.data.Dataset.write_databricks_table(output, \n",
    "#                                         f\"{catalog_name}.{schema_name}.f'{lance_table_name}_results\",\n",
    "#                                          mode='overwrite')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "CLUSTER_ray_actor_pool_shards",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
