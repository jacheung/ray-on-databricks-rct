{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c350411-649e-44e4-92e2-b5fc3f20bb74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install ray[all]\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29a0d83e-12f0-4fdf-8d95-328cfbaab549",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Shard plan\n",
    "1. Create parquet shards (e.g., 10) --> See 00a_shard_data_preparation\n",
    "2. Construct inferencing Ray actors\n",
    "  * Ray actor, as input, receive shard num.\n",
    "  * Ray actor build local lance DB instance using shard from parquet\n",
    "3. inference:\n",
    "  * ray.data a batch (e.g. 1k)\n",
    "  * rerank?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5037d8bf-6c60-4167-863e-5f534f5f76ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog_name = 'jon_cheung'\n",
    "schema_name = 'vizio_poc'\n",
    "lance_table_name = 'audio_100M_chunk_shard_1'\n",
    "num_test_vectors = 2_000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2577209-7193-40c7-9d33-3b0aeb9ba388",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.util.spark import setup_ray_cluster, shutdown_ray_cluster\n",
    "\n",
    "restart = True\n",
    "if restart is True:\n",
    "  try:\n",
    "    shutdown_ray_cluster()\n",
    "  except:\n",
    "    pass\n",
    "  try:\n",
    "    ray.shutdown()\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "setup_ray_cluster(\n",
    "  min_worker_nodes=2,\n",
    "  max_worker_nodes=4,\n",
    "  num_cpus_worker_node=48,\n",
    "  num_gpus_worker_node=0,\n",
    "  collect_log_to_path=\"/dbfs/Users/jon.cheung@databricks.com/ray_collected_logs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4d23297-c1ac-48c8-9cd1-0f9c648d6569",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "import lancedb\n",
    "\n",
    "\n",
    "class LanceDBActor:\n",
    "    def __init__(self, parquet_path, pyarrow_schema):\n",
    "        \n",
    "        # TODO: consider parameterizing\n",
    "        lance_db_uri = \"/tmp/lancedb\"\n",
    "        num_partitions = 10\n",
    "        num_sub_vectors = 5\n",
    "        \n",
    "        # Create lanceDB directory\n",
    "        os.makedirs(lance_db_uri, exist_ok=True)\n",
    "\n",
    "        # assuming 96 core CPU...\n",
    "        os.environ[\"LANCE_CPU_THREADS\"] = \"48\"\n",
    "        os.environ[\"LANCE_IO_THREADS\"] = \"48\"\n",
    "\n",
    "        db = lancedb.connect(lance_db_uri)\n",
    "        self.table_arrow = db.create_table(lance_table_name,\n",
    "                                      data=LanceDBActor._get_batches_from_parquet_with_progress(parquet_path,\n",
    "                                                                                                pyarrow_schema,\n",
    "                                                                                                batch_size=200_000),\n",
    "                                      mode=\"overwrite\"\n",
    "                                      )\n",
    "        \n",
    "        self.table_arrow.create_index(\n",
    "                metric=\"l2\",\n",
    "                vector_column_name=\"list_col\",\n",
    "                num_partitions=num_partitions,\n",
    "                num_sub_vectors=num_sub_vectors\n",
    "            )\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def _get_batches_from_parquet_with_progress(parquet_path: str,\n",
    "                                                schema: pa.Schema,\n",
    "                                                 batch_size: int = 4096):\n",
    "            \"\"\"\n",
    "            Reads a Parquet file in chunks and yields PyArrow RecordBatches,\n",
    "            displaying progress using tqdm.\n",
    "            \"\"\"\n",
    "        dataset = ds.dataset(parquet_path, format=\"parquet\", schema=schema)\n",
    "\n",
    "        total_rows = dataset.count_rows()\n",
    "        total_batches = np.ceil(total_rows / batch_size)\n",
    "        scanner = dataset.scanner(batch_size=batch_size)\n",
    "\n",
    "        # Wrap the scanner.to_batches() with tqdm\n",
    "        # We use `total_batches` for tqdm's 'total' argument.\n",
    "        with tqdm(total=total_batches, unit=\"batch\", desc=\"Ingesting Parquet Batches\") as pbar:\n",
    "            for batch in scanner.to_batches():\n",
    "                yield batch\n",
    "                pbar.update(1) # Manually update progress for each yielded batch\n",
    "                pbar.set_postfix({\"rows_in_batch\": len(batch)})\n",
    "\n",
    "    def __call__(self, batch: np.ndarray, limit: int):\n",
    "        results = self.table_arrow.search(query_batch).limit(limit).to_pandas()\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "## TODO: Can actors read from Volumes?! Do we need an S3 bucket instead?\n",
    "audio_parquet_path = f'/Volumes/{catalog}/{schema}/{lance_table_name}'\n",
    "pyarrow_schema = pa.schema(\n",
    "    [\n",
    "        pa.field(\"id\", pa.int64()),\n",
    "        pa.field(\"list_col\", pa.list_(pa.float16(), 35)),   # Fixed size list\n",
    "    ]\n",
    ")\n",
    "\n",
    "def create_arrays(n, dimensions):\n",
    "    return [np.random.randint(0, 256, size=dimensions).astype(np.float16) for _ in range(n)]\n",
    "\n",
    "# Make Ray Data dataset and inference with it\n",
    "large_query_batch =  ray.data.from_items(create_arrays(num_test_vectors, dimensions=35))\n",
    "large_query_batch.map_batches(LanceDBActor,\n",
    "                              fn_constructor_args={'parquet_path': audio_parquet_path, \n",
    "                                                   'pyarrow_schema': pyarrow_schema},\n",
    "                              fn_args={'limit': 1},\n",
    "                              num_cpus=48,\n",
    "                              batch_size=250)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01a_ray_actor_pool_shards",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
