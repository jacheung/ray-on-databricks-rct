{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d285a79-690f-441b-a08c-bf5a664c5041",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Method 1: Every actor holds a full replica of the dataset\n",
    "routing handled by actor pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd20105f-df0a-48b8-97df-2d624ed88ef0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Ray Data"
    }
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "import lancedb\n",
    "\n",
    "num_test_vectors = 1_000_000\n",
    "\n",
    "class LanceDBActor:\n",
    "    def __init__(self, parquet_path, pyarrow_schema):\n",
    "        \n",
    "        # TODO: consider parameterizing\n",
    "        num_partitions = 200\n",
    "        num_sub_vectors = 5\n",
    "        lance_db_uri = \"/tmp/lancedb\"\n",
    "        # assuming 96 core CPU...\n",
    "        os.environ[\"LANCE_CPU_THREADS\"] = \"96\"\n",
    "        os.environ[\"LANCE_IO_THREADS\"] = \"96\"\n",
    "\n",
    "        db = lancedb.connect(lance_db_uri)\n",
    "        self.table_arrow = db.create_table(lance_table_name,\n",
    "                                      data=_get_batches_from_parquet_with_progress(parquet_path,\n",
    "                                                                                    pyarrow_schema,\n",
    "                                                                                     batch_size=200_000),\n",
    "                                      mode=\"overwrite\"\n",
    "                                      )\n",
    "        \n",
    "        self.table_arrow.create_index(\n",
    "                metric=\"l2\",\n",
    "                vector_column_name=\"list_col\",\n",
    "                num_partitions=num_partitions,\n",
    "                num_sub_vectors=num_sub_vectors\n",
    "            )\n",
    "        print(\"Index loaded.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_batches_from_parquet_with_progress(parquet_path: str,\n",
    "                                                schema: pa.Schema,\n",
    "                                                 batch_size: int = 4096):\n",
    "            \"\"\"\n",
    "            Reads a Parquet file in chunks and yields PyArrow RecordBatches,\n",
    "            displaying progress using tqdm.\n",
    "            \"\"\"\n",
    "        dataset = ds.dataset(parquet_path, format=\"parquet\", schema=schema)\n",
    "\n",
    "        total_rows = dataset.count_rows()\n",
    "        total_batches = np.ceil(total_rows / batch_size)\n",
    "        scanner = dataset.scanner(batch_size=batch_size)\n",
    "\n",
    "        # Wrap the scanner.to_batches() with tqdm\n",
    "        # We use `total_batches` for tqdm's 'total' argument.\n",
    "        with tqdm(total=total_batches, unit=\"batch\", desc=\"Ingesting Parquet Batches\") as pbar:\n",
    "            for batch in scanner.to_batches():\n",
    "                yield batch\n",
    "                pbar.update(1) # Manually update progress for each yielded batch\n",
    "                pbar.set_postfix({\"rows_in_batch\": len(batch)})\n",
    "\n",
    "    def __call__(self, batch: np.ndarray, limit: int):\n",
    "        results = self.table_arrow.search(query_batch).limit(limit).to_pandas()\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "## TODO: Can actors read from Volumes?! Do we just need some S3 bucket instead?\n",
    "audio_parquet_path = f'/Volumes/{catalog}/{schema}/{lance_table_name}'\n",
    "pyarrow_schema = pa.schema(\n",
    "    [\n",
    "        pa.field(\"id\", pa.int64()),\n",
    "        pa.field(\"list_col\", pa.list_(pa.float16(), 35)),   # Fixed size list\n",
    "    ]\n",
    ")\n",
    "\n",
    "def create_arrays(n, dimensions):\n",
    "    return [np.random.randint(0, 256, size=dimensions).astype(np.float16) for _ in range(n)]\n",
    "\n",
    "\n",
    "\n",
    "# Make Ray Data dataset and inference with it\n",
    "large_query_batch =  ray.data.from_items(create_arrays(num_test_vectors, dimensions=35))\n",
    "large_query_batch.map_batches(LanceDBActor,\n",
    "                              fn_constructor_args={'parquet_path': audio_parquet_path, \n",
    "                                                   'pyarrow_schema': pyarrow_schema},\n",
    "                              fn_args={'limit': 1},\n",
    "                              num_cpus=96,\n",
    "                              memory=1.5e+12)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eaa377ad-105d-48f9-99f9-424c73fc55e0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Ray core primitives"
    }
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "import lancedb\n",
    "\n",
    "@ray.core\n",
    "class LanceDBActor:\n",
    "    def __init__(self, parquet_path, pyarrow_schema):\n",
    "        \n",
    "        # TODO: consider parameterizing\n",
    "        num_partitions = 200\n",
    "        num_sub_vectors = 5\n",
    "        lance_db_uri = \"/tmp/lancedb\"\n",
    "        # assuming 96 core CPU...\n",
    "        os.environ[\"LANCE_CPU_THREADS\"] = \"80\"\n",
    "        os.environ[\"LANCE_IO_THREADS\"] = \"32\"\n",
    "\n",
    "        db = lancedb.connect(lance_db_uri)\n",
    "        self.table_arrow = db.create_table(lance_table_name,\n",
    "                                      data=_get_batches_from_parquet_with_progress(parquet_path,\n",
    "                                                                                    pyarrow_schema,\n",
    "                                                                                     batch_size=200_000),\n",
    "                                      mode=\"overwrite\"\n",
    "                                      )\n",
    "        \n",
    "        self.table_arrow.create_index(\n",
    "                metric=\"l2\",\n",
    "                vector_column_name=\"list_col\",\n",
    "                num_partitions=num_partitions,\n",
    "                num_sub_vectors=num_sub_vectors\n",
    "            )\n",
    "        print(\"Index loaded.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_batches_from_parquet_with_progress(parquet_path: str,\n",
    "                                                schema: pa.Schema,\n",
    "                                                 batch_size: int = 4096):\n",
    "            \"\"\"\n",
    "            Reads a Parquet file in chunks and yields PyArrow RecordBatches,\n",
    "            displaying progress using tqdm.\n",
    "            \"\"\"\n",
    "        dataset = ds.dataset(parquet_path, format=\"parquet\", schema=schema)\n",
    "\n",
    "        total_rows = dataset.count_rows()\n",
    "        total_batches = np.ceil(total_rows / batch_size)\n",
    "        scanner = dataset.scanner(batch_size=batch_size)\n",
    "\n",
    "        # Wrap the scanner.to_batches() with tqdm\n",
    "        # We use `total_batches` for tqdm's 'total' argument.\n",
    "        with tqdm(total=total_batches, unit=\"batch\", desc=\"Ingesting Parquet Batches\") as pbar:\n",
    "            for batch in scanner.to_batches():\n",
    "                yield batch\n",
    "                pbar.update(1) # Manually update progress for each yielded batch\n",
    "                pbar.set_postfix({\"rows_in_batch\": len(batch)})\n",
    "\n",
    "    def search(self, query_batch: np.ndarray, limit: int):\n",
    "        results = self.table_arrow.search(query_batch).limit(limit).to_pandas()\n",
    "\n",
    "        return results\n",
    "\n",
    "# --- Main Application ---\n",
    "# 1. Create Actor Pool// non-autoscalable\n",
    "num_replicas = 64\n",
    "\n",
    "## TODO: Can actors read from Volumes?! Do we just need some S3 bucket instead?\n",
    "audio_parquet_path = f'/Volumes/{catalog}/{schema}/{lance_table_name}'\n",
    "pyarrow_schema = pa.schema(\n",
    "    [\n",
    "        pa.field(\"id\", pa.int64()),\n",
    "        pa.field(\"list_col\", pa.list_(pa.float16(), 35)),   # Fixed size list\n",
    "    ]\n",
    ")\n",
    "\n",
    "actors = [LanceDBActor.remote(audio_parquet_path, pyarrow_schema) for _ in range(num_replicas)]\n",
    "pool = ray.util.ActorPool(actors)\n",
    "\n",
    "# 2. Divide a large batch of queries and submit\n",
    "def create_arrays(n, dimensions):\n",
    "    return [np.random.randint(0, 256, size=dimensions).astype(np.float16) for _ in range(n)]\n",
    "\n",
    "num_test_vectors = 1_000_000\n",
    "\n",
    "# map_unordered is efficient, yielding results as they complete\n",
    "large_query_batch = create_arrays(num_test_vectors, 35)\n",
    "results_generator = pool.map_unordered(lambda actor, batch: actor.search.remote(batch, limit=1),\n",
    "                                       [large_query_batch])\n",
    "\n",
    "for result in results_generator:\n",
    "    \n",
    "    # write results to spark\n",
    "    pass\n",
    "# Make Ray Data dataset.\n",
    "# large_query_batch =  ray.data.from_items(create_arrays(num_test_vectors, dimensions=35))\n",
    "# large_query_batch.map_batches(LanceDBActor, concurrency=10)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07f485a2-2bc4-4fd9-b5bc-5fc041de4ce9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Method 2: routing to shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e43e7207-b95a-41a1-a7cd-e056bafcb23d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "from qdrant_client import QdrantClient\n",
    "# Assume 30 shard endpoints are configured\n",
    "QDRANT_ENDPOINTS = [\"host1:6333\", \"host2:6333\", ..., \"host30:6333\"]\n",
    "\n",
    "@ray.remote\n",
    "class QueryRouter:\n",
    "    def __init__(self):\n",
    "        # This actor holds persistent connections to all DB shards\n",
    "        self.clients = [QdrantClient(host=endpoint) for endpoint in QDRANT_ENDPOINTS]\n",
    "        print(\"Initialized connections to all shards.\")\n",
    "\n",
    "    def _get_shard_index(self, query_metadata):\n",
    "        # Simple example: determine shard from a 'user_id'\n",
    "        # A more robust implementation would use consistent hashing\n",
    "        user_id = query_metadata.get(\"user_id\", 0)\n",
    "        return user_id % len(self.clients)\n",
    "\n",
    "    async def search(self, vector, query_metadata, top_k):\n",
    "        shard_index = self._get_shard_index(query_metadata)\n",
    "        client = self.clients[shard_index]\n",
    "\n",
    "        # Asynchronously query the specific database shard\n",
    "        # This is a non-blocking operation\n",
    "        return await client.search(\n",
    "            collection_name=\"my_collection\",\n",
    "            query_vector=vector,\n",
    "            limit=top_k\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6682f359-4a7a-4110-8bf4-79750f52e988",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "import numpy as np\n",
    "# Assume Faiss is used for the index\n",
    "# import faiss\n",
    "\n",
    "@ray.remote\n",
    "class ShardActor:\n",
    "    def __init__(self, shard_id: int, total_shards: int):\n",
    "        # Each actor loads only its assigned shard of the data\n",
    "        index_path = f\"s3://my-bucket/shard_{shard_id}_of_{total_shards}.faiss\"\n",
    "        print(f\"Actor {shard_id} loading {index_path}...\")\n",
    "        # self.index = faiss.read_index(index_path)\n",
    "        self.shard_id = shard_id\n",
    "        print(f\"Actor {shard_id} ready.\")\n",
    "\n",
    "    def search(self, query_vector: np.ndarray, k: int):\n",
    "        # Search the local shard index\n",
    "        # distances, local_indices = self.index.search(query_vector, k)\n",
    "        # You must map local_indices back to global IDs\n",
    "        # global_indices = self.map_local_to_global(local_indices)\n",
    "        # return (distances, global_indices)\n",
    "        # Placeholder for demonstration\n",
    "        return (np.random.rand(query_vector.shape[0], k),\n",
    "                np.random.randint(0, 3_000_000_000, size=(query_vector.shape[0], k)))\n",
    "\n",
    "\n",
    "# --- Main Application ---\n",
    "# 1. Create one actor for each shard\n",
    "num_shards = 30\n",
    "shard_actors = [ShardActor.remote(i, num_shards) for i in range(num_shards)]\n",
    "\n",
    "# 2. Broadcast a single query to ALL shards\n",
    "query_vector = np.random.rand(1, 768).astype('float32')\n",
    "k = 10\n",
    "\n",
    "# Fan-out the query to all actors\n",
    "results_futures = [actor.search.remote(query_vector, k) for actor in shard_actors]\n",
    "all_shard_results = ray.get(results_futures) # Returns a list of [(distances, indices), ...]\n",
    "\n",
    "# 3. Merge results from all shards\n",
    "# This is a critical step. You need to collect all candidates and re-rank them.\n",
    "all_distances = np.concatenate([res[0][0] for res in all_shard_results])\n",
    "all_indices = np.concatenate([res[1][0] for res in all_shard_results])\n",
    "\n",
    "# Sort by distance and take the top k\n",
    "top_k_indices = np.argsort(all_distances)[:k]\n",
    "final_indices = all_indices[top_k_indices]\n",
    "final_distances = all_distances[top_k_indices]\n",
    "\n",
    "print(\"Final Top-K Indices:\", final_indices)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "ray_actor_pool_full_replica",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
