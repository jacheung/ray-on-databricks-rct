{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e809f69b-44a6-4b71-a09a-979411e26579",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Distributed XGBoost with GPUs on Ray \n",
    "\n",
    "Ray provides a version of XGBoost to perform distributed data parallelism. With drop-in replacements of `xgboost` native classes, XGboost Ray allows you to leverage multi-node clusters to distribute your training. \n",
    "\n",
    "This demo uses a dataset created from `00-create-dataset` with 100M rows x 100 features columns x 1 target column (5 classes) for multi-class classification. This dataset is ~40GiB. \n",
    "\n",
    "\n",
    "#### FAQs\n",
    "When do I switch to a distributed version of XGBoost? \n",
    "- XGboost datasets > than 1B rows should use distributed data parallelism (DDP). I'm only using 100M rows here for demonstration purposes. \n",
    "- Consider single-node and multi-threading across all CPUs, then DDP across multiple nodes with CPUs, then DDP leveraging multiple GPUs. \n",
    "\n",
    "If I'm using GPUs, how much memory (VRAM) do I need for my dataset? A quick behind the napkin math:\n",
    "- 100M rows x 100 columns x 4 bytes (float16) = ~40GB \n",
    "- We'll need a total of 2-4x the data footprint in VRAM across our GPUs (we'll go with 2x so ~80GiB) to train our model. This extra is to account for the boosting rounds, size of the model, etc...\n",
    "    - Depending on `num_boost_round` (a.k.a. `num_estimators`) or `max_depth` you may require more. \n",
    "    - I'm using nodes with A10G (each GPU has 24GB VRAM). With that said, I'll need about 3 GPUs to train one model.\n",
    "\n",
    "\n",
    "#### Compute specifications to run this notebook\n",
    "```json\n",
    "{\n",
    "    \"num_workers\": 8,\n",
    "    \"cluster_name\": \"Multi-node MLR w/ GPUs\",\n",
    "    \"spark_version\": \"16.4.x-gpu-ml-scala2.13\",\n",
    "    \"spark_conf\": {\n",
    "        \"spark.task.resource.gpu.amount\": \"0\",\n",
    "        \"spark.executor.memory\": \"1g\"\n",
    "    },\n",
    "    \"aws_attributes\": {\n",
    "        \"first_on_demand\": 1,\n",
    "        \"availability\": \"SPOT_WITH_FALLBACK\",\n",
    "        \"zone_id\": \"auto\",\n",
    "        \"spot_bid_price_percent\": 100,\n",
    "        \"ebs_volume_count\": 0\n",
    "    },\n",
    "    \"node_type_id\": \"g5.8xlarge\",\n",
    "    \"driver_node_type_id\": \"g5.8xlarge\",\n",
    "    \"autotermination_minutes\": 60,\n",
    "    \"enable_elastic_disk\": false,\n",
    "    \"single_user_name\": \"jon.cheung@databricks.com\",\n",
    "    \"enable_local_disk_encryption\": false,\n",
    "    \"data_security_mode\": \"SINGLE_USER\",\n",
    "    \"runtime_engine\": \"STANDARD\",\n",
    "    \"assigned_principal\": \"user:jon.cheung@databricks.com\",\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45f9e867-2d65-4987-ac74-c84bc5b8a134",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -qU ray[all]=2.48.0 xgboost optuna\n",
    "%pip install '/Workspace/Users/jon.cheung@databricks.com/ray-on-databricks-rct/distributed-training/XGBoost/databricks.serverless_gpu-0.5.3-py3-none-any.whl'\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bdc7994-3c80-47bd-a10c-19038157cb75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "num_training_rows = 101_000_000\n",
    "num_training_columns = 100\n",
    "num_labels = 5\n",
    "catalog = \"main\"\n",
    "schema = \"ray_gtm_examples\"\n",
    "\n",
    "table = f\"synthetic_data_{num_training_rows}_rows_{num_training_columns}_columns_{num_labels}_labels\"\n",
    "label=\"target\"\n",
    "mlflow_experiment_name = f\"/Users/jon.cheung@databricks.com/ray_xgboost\"\n",
    "\n",
    "# If running in a multi-node cluster, this is where you\n",
    "# should configure the run's persistent storage that is accessible\n",
    "# across all worker nodes.\n",
    "ray_xgboost_path = '/dbfs/Users/jon.cheung@databricks.com/ray_xgboost/' \n",
    "# This is for stashing the cluster logs\n",
    "ray_logs_path = \"/dbfs/Users/jon.cheung@databricks.com/ray_collected_logs/\"\n",
    "\n",
    "# Set the parameters here so mlflow works properly at the Ray head + worker nodes\n",
    "os.environ['DATABRICKS_HOST'] = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get()\n",
    "os.environ['DATABRICKS_TOKEN'] = dbutils.secrets.get(scope = \"development\", key = \"jon_cheung_PAT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4cb052f-751e-438f-b3cc-c9c5ddcb14e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "def read_ray_dataset(catalog, schema, table):\n",
    "  try: \n",
    "    ## Option 1 (PREFERRED): Build a Ray Dataset using a Databricks SQL Warehouse\n",
    "    # Insert your SQL warehouse ID here. I've queried my 100M row dataset using a Small t-shirt sized cluster.\n",
    "\n",
    "    # Ensure you've set the DATABRICKS_TOKEN so you can query using the warehouse compute\n",
    "    ds = ray.data.read_databricks_tables(\n",
    "      warehouse_id='2a72600bb68f00ee',\n",
    "      catalog=catalog,\n",
    "      schema=schema,\n",
    "      query=f'SELECT * FROM {table}',\n",
    "    )\n",
    "    # Testing out read from UC. However, this requires turning off deletion vectors\n",
    "    # ds = ray.data.read_unity_catalog(f'{catalog}.{schema}.{table}',\n",
    "    #                                url='https://e2-demo-field-eng.cloud.databricks.com/',\n",
    "    #                                token=os.environ['DATABRICKS_TOKEN'],\n",
    "    #                                region='us-west-2',\n",
    "    #                                reader_kwargs={\"override_num_blocks\": 1000})\n",
    "    print('read directly from UC')\n",
    "  except: \n",
    "    ## Option 2: Build a Ray Dataset using a Parquet files\n",
    "    # If you have too many Ray nodes, you may not be able to create a Ray dataset using the warehouse method above because of rate limits. One back up solution is to create parquet files from the delta table and build a ray dataset from that. This is not the recommended route because, in essence, you are duplicating data.\n",
    "    parquet_path = f'/Volumes/{catalog}/{schema}/synthetic_data/{table}'\n",
    "    ds = ray.data.read_parquet(parquet_path)\n",
    "    print('read directly from parquet')\n",
    "\n",
    "  train_dataset, val_dataset = ds.train_test_split(test_size=0.25)\n",
    "  return train_dataset, val_dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3060fa40-7b73-4524-8533-397e17f5c7fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import xgboost\n",
    "import ray.train\n",
    "from ray.train.xgboost import XGBoostTrainer, RayTrainReportCallback\n",
    "\n",
    "def train_fn_per_worker(params: dict):\n",
    "    \"\"\"\n",
    "    Trains an XGBoost model on a shard of the distributed dataset assigned to this worker.\n",
    "\n",
    "    This should look very similar to a vanilla XGboost training.\n",
    "\n",
    "    This function is designed to be executed by individual Ray Train workers.\n",
    "    It retrieves the training and validation data shards, converts them to DMatrix format,\n",
    "    and performs a portion of the distributed XGBoost training. Ray Train handles\n",
    "    the inter-worker communication.\n",
    "\n",
    "    Args:\n",
    "        params (dict): A dictionary of XGBoost training parameters, including\n",
    "                       'num_estimators', 'eval_metric', and potentially other\n",
    "                       XGBoost-specific parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get dataset shards for this worker\n",
    "    train_shard = ray.train.get_dataset_shard(\"train\")\n",
    "    val_shard = ray.train.get_dataset_shard(\"val\")\n",
    "\n",
    "    # Convert shards to pandas DataFrames\n",
    "    train_df = train_shard.materialize().to_pandas()\n",
    "    val_df = val_shard.materialize().to_pandas()\n",
    "\n",
    "    train_X = train_df.drop(label, axis=1)\n",
    "    train_y = train_df[label]\n",
    "    val_X = val_df.drop(label, axis=1)\n",
    "    val_y = val_df[label]\n",
    "    \n",
    "    dtrain = xgboost.DMatrix(train_X, label=train_y)\n",
    "    deval = xgboost.DMatrix(val_X, label=val_y)\n",
    "\n",
    "    # Do distributed data-parallel training.\n",
    "    # Ray Train sets up the necessary coordinator processes and\n",
    "    # environment variables for workers to communicate with each other.\n",
    "    evals_results = {}\n",
    "    bst = xgboost.train(\n",
    "        params,\n",
    "        dtrain=dtrain,\n",
    "        evals=[(deval, \"validation\")],\n",
    "        num_boost_round=params['num_estimators'],\n",
    "        evals_result=evals_results,\n",
    "        # early_stopping_rounds=params['early_stopping_rounds'],\n",
    "        callbacks=[RayTrainReportCallback(metrics={params['eval_metric']: f\"validation-{params['eval_metric']}\"},\n",
    "                                          frequency=1)],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2af42c58-c7c5-4b68-b13c-ee62eb956946",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def train_driver_fn(config: dict, train_dataset, val_dataset):\n",
    "    \"\"\"\n",
    "    Drives the distributed XGBoost training process using Ray Train.\n",
    "\n",
    "    This function sets up the XGBoostTrainer, configures scaling (number of workers, GPU usage,\n",
    "    and resources per worker), and initiates the distributed training by calling `trainer.fit()`.\n",
    "    It also propagates metrics back to Ray Tune if integrated.\n",
    "\n",
    "    Args:\n",
    "        config (dict): A dictionary containing run-level hyperparameters such as\n",
    "                       'num_workers', 'use_gpu', and a nested 'params' dictionary\n",
    "                       for XGBoost training parameters.\n",
    "        train_dataset: The Ray Dataset for training.\n",
    "        val_dataset: The Ray Dataset for validation.\n",
    "\n",
    "    Returns:\n",
    "        None: The function reports metrics to Ray Tune but does not explicitly return a value.\n",
    "              The trained model artifact is typically handled by Ray Train's checkpointing\n",
    "              or by the `train_fn_per_worker` if saved directly.\n",
    "    \"\"\"\n",
    "    # Unpack run-level hyperparameters.\n",
    "    num_workers = config[\"num_workers\"]\n",
    "    use_gpu = config[\"use_gpu\"]\n",
    "    params = config['params']\n",
    "\n",
    "    # Initialize the XGBoostTrainer, which orchestrates the distributed training using Ray.\n",
    "    trainer = XGBoostTrainer(\n",
    "      train_loop_per_worker=train_fn_per_worker, # The function to be executed on each worker\n",
    "      train_loop_config=params,\n",
    "      # By default Ray uses 1 GPU and 1 CPU per worker if resources_per_worker is not specified.\n",
    "      # XGBoost is multi-threaded, so multiple CPUs can be assigned per worker, but not GPUs.\n",
    "      scaling_config=ray.train.ScalingConfig(num_workers=num_workers, \n",
    "                                             use_gpu=use_gpu,\n",
    "                                             resources_per_worker={\"CPU\": 24, \"GPU\": 1}),\n",
    "      datasets={\"train\": train_dataset, \"val\": val_dataset},  # Ray Datasets to be used by the trainer + workers\n",
    "      run_config=ray.train.RunConfig(storage_path=ray_xgboost_path,                                  \n",
    "                                    #  name=f\"train-trial_id={ray.tune.get_context().get_trial_id()}\"\n",
    "                                    )\n",
    "    )\n",
    "    \n",
    "                                    \n",
    "    result = trainer.fit()\n",
    "    \n",
    "    # Propagate metrics back up for Ray Tune. \n",
    "    # Ensure 'mlogloss' is the correct metric key based on your eval_metric and results.\n",
    "    ray.tune.report({params['eval_metric']: result.metrics['mlogloss']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10ebc789-9d69-4a56-9c14-4bb6611bbf31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# with test.checkpoint.as_directory() as checkpoint_dir:\n",
    "#     model_path = os.path.join(checkpoint_dir, RayTrainReportCallback.CHECKPOINT_NAME)\n",
    "#     print(model_path)\n",
    "#     model = xgboost.Booster()\n",
    "#     model.load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4c23f41-5112-43ed-a668-7a284034eb3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Ray Tune with Ray Train and Mlflow Integration\n",
    "\n",
    "https://docs.ray.io/en/latest/train/user-guides/hyperparameter-optimization.html#hyperparameter-tuning-with-ray-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35128cfc-d991-482b-84ff-7dcc49c7274e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.tuner import Tuner\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from ray.air.integrations.mlflow import MLflowLoggerCallback\n",
    "from serverless_gpu.ray import ray_launch \n",
    "\n",
    "@ray.remote(num_cpus=1)  # please make sure main_task is not scheduled on head\n",
    "class TaskRunner:\n",
    "    def run(self):\n",
    "      print(\"resources: \" , ray.cluster_resources())\n",
    "      \n",
    "      # Load dataset as distributed Ray Dataset\n",
    "      train_dataset, val_dataset = read_ray_dataset(catalog, schema, table)\n",
    "      \n",
    "      # Define resources per HPO trial and calculate max concurrent HPO trials\n",
    "      num_workers = 1\n",
    "      num_hpo_trials = 20\n",
    "      resources = ray.cluster_resources()\n",
    "      total_cluster_gpus = resources.get(\"GPU\") \n",
    "      # max_concurrent_trials = int(total_cluster_gpus // num_gpu_workers_per_trial)\n",
    "      max_concurrent_trials = 7\n",
    "\n",
    "\n",
    "      # Define the hyperparameter search space.\n",
    "      # XGB sample hyperparameter configs\n",
    "      param_space = {\n",
    "          \"num_workers\": num_workers,\n",
    "          \"use_gpu\": True,\n",
    "          \"params\":{\"objective\": \"multi:softmax\",\n",
    "                    'eval_metric': 'mlogloss', \n",
    "                    \"tree_method\": \"hist\",\n",
    "                    \"device\": \"cuda\",\n",
    "                    \"num_class\": num_labels,\n",
    "                    \"learning_rate\": tune.uniform(0.01, 0.3),\n",
    "                    \"num_estimators\": tune.randint(25, 50)}\n",
    "      }\n",
    "\n",
    "      # # Set up search algorithm. Here we use Optuna and use the default the Bayesian sampler (i.e. TPES)\n",
    "      optuna = OptunaSearch(metric=param_space['params']['eval_metric'], \n",
    "                            mode=\"min\")\n",
    "\n",
    "      # Set up Tuner job and run.\n",
    "      tuner = tune.Tuner(\n",
    "          tune.with_parameters(train_driver_fn,\n",
    "                              train_dataset = train_dataset,\n",
    "                              val_dataset = val_dataset),\n",
    "          run_config=tune.RunConfig(name='mlflow',\n",
    "                                  #   callbacks=[MLflowLoggerCallback(\n",
    "                                  #       experiment_name=mlflow_experiment_name,\n",
    "                                  #       save_artifact=True,\n",
    "                                  #       log_params_on_trial_end=True)]\n",
    "                                    ),\n",
    "          tune_config=tune.TuneConfig(num_samples=num_hpo_trials,\n",
    "                                      max_concurrent_trials=max_concurrent_trials,\n",
    "                                      search_alg=optuna,\n",
    "                                      ),\n",
    "          param_space=param_space,\n",
    "\n",
    "          )\n",
    "\n",
    "      results = tuner.fit()\n",
    "      best_params = results.get_best_result(metric=param_space['params']['eval_metric'], \n",
    "                          mode=\"min\").config\n",
    "\n",
    "      print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51753b4f-dd69-447c-b52a-96009438c4d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import ray\n",
    "# from ray import tune\n",
    "# from ray.tune.tuner import Tuner\n",
    "# from ray.tune.search.optuna import OptunaSearch\n",
    "# from ray.air.integrations.mlflow import MLflowLoggerCallback\n",
    "# from serverless_gpu.ray import ray_launch \n",
    "# @ray.remote(num_cpus=1)  # please make sure main_task is not scheduled on head\n",
    "# class TaskRunner:\n",
    "#     def run(self):\n",
    "#       print(\"resources: \" , ray.cluster_resources())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b014a996-b6e7-4721-82bf-d38b40899866",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@ray_launch(gpus=8, gpu_type='H100', remote=True)\n",
    "def my_ray_function():\n",
    "    print(\"resources: \" , ray.cluster_resources())\n",
    "    runner = TaskRunner.remote()\n",
    "    ray.get(runner.run.remote())\n",
    "    # create a timeline trace file to analyze the performance\n",
    "    # timeline_json_file = config.ray_init.get(\"timeline_json_file\", None)\n",
    "    # if timeline_json_file:\n",
    "    #     ray.timeline(filename=timeline_json_file)\n",
    "\n",
    "my_ray_function.distributed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f9dfb64-ee1a-44b7-893d-83f09bc4c851",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# @ray_launch(gpus=1, gpu_type='a10', remote=True)\n",
    "# def my_ray_function():\n",
    "#     # Load dataset as distributed Ray Dataset\n",
    "#     ray_ds = read_ray_dataset(catalog, schema, table)\n",
    "\n",
    "#     output = ray_ds.map_groups(resources={'GPU':.25})\n",
    "\n",
    "#     ray.data.write_parquet(output)\n",
    "\n",
    "# my_ray_function.distributed()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3397375242701867,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "02-train-distributed-xgboost-SGC",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
