{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e809f69b-44a6-4b71-a09a-979411e26579",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Distributed XGBoost with GPUs on Ray \n",
    "\n",
    "Ray provides a version of XGBoost to perform distributed data parallelism. With drop-in replacements of `xgboost` native classes, XGboost Ray allows you to leverage multi-node clusters to distribute your training. \n",
    "\n",
    "This demo uses a dataset created from `00-create-dataset` with 100M rows x 100 features columns x 1 target column (5 classes) for multi-class classification. This dataset is ~40GiB. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### FAQs\n",
    "When do I switch to a distributed version of XGBoost? \n",
    "- XGboost datasets > than 1B rows should use distributed data parallelism (DDP). I'm only using 100M rows here for demonstration purposes. \n",
    "- Consider using a single-node and multi-threading across all CPUs before switching to distributed training leveraging multi-GPUs. \n",
    "\n",
    "How much memory (VRAM) do I need for my dataset? A quick behind the napkin math:\n",
    "- 100M rows x 100 columns x 4 bytes (float16) = ~40GB \n",
    "- We'll need between 1-3x the data footprint in VRAM across our GPUs (we'll go with 2x so ~80GiB) to train our model (accounting for gradients, model). \n",
    "- depending on `num_boost_round` (a.k.a. `num_estimators`) or `max_depth` you may require 4x-8x more memory per node. \n",
    "- I'm using g4dn.12xlarge worker nodes on AWS (4 GPUs/node at 16GiB VRAM/GPU). With that said, I'll need about 6 GPUs to train one model.\n",
    "\n",
    "\n",
    "#### Compute specifications to run this notebook\n",
    "```json\n",
    "{\n",
    "    \"num_workers\": 2,\n",
    "    \"cluster_name\": \"Multi-node MLR w/ GPUs\",\n",
    "    \"spark_version\": \"16.4.x-gpu-ml-scala2.13\",\n",
    "    \"spark_conf\": {\n",
    "        \"spark.task.resource.gpu.amount\": \"0\",\n",
    "        \"spark.executor.memory\": \"1g\"\n",
    "    },\n",
    "    \"aws_attributes\": {\n",
    "        \"first_on_demand\": 1,\n",
    "        \"availability\": \"SPOT_WITH_FALLBACK\",\n",
    "        \"zone_id\": \"auto\",\n",
    "        \"spot_bid_price_percent\": 100,\n",
    "        \"ebs_volume_count\": 0\n",
    "    },\n",
    "    \"node_type_id\": \"g4dn.12xlarge\",\n",
    "    \"driver_node_type_id\": \"g4dn.12xlarge\",\n",
    "    \"spark_env_vars\": {\n",
    "        \"DATABRICKS_TOKEN\": \"{{secrets/development/jon_cheung_PAT}}\"\n",
    "    },\n",
    "    \"enable_elastic_disk\": false,\n",
    "    \"single_user_name\": \"jon.cheung@databricks.com\",\n",
    "    \"enable_local_disk_encryption\": false,\n",
    "    \"data_security_mode\": \"SINGLE_USER\",\n",
    "    \"runtime_engine\": \"STANDARD\",\n",
    "    \"assigned_principal\": \"user:jon.cheung@databricks.com\",\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45f9e867-2d65-4987-ac74-c84bc5b8a134",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -qU ray[all]=2.47.1\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bdc7994-3c80-47bd-a10c-19038157cb75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "num_training_rows = 100_000_000\n",
    "num_training_columns = 100\n",
    "num_labels = 5\n",
    "\n",
    "catalog = \"main\"\n",
    "schema = \"ray_gtm_examples\"\n",
    "\n",
    "table = f\"synthetic_data_{num_training_rows}_rows_{num_training_columns}_columns_{num_labels}_labels\"\n",
    "label=\"target\"\n",
    "\n",
    "# If running in a multi-node cluster, this is where you\n",
    "# should configure the run's persistent storage that is accessible\n",
    "# across all worker nodes.\n",
    "ray_xgboost_path = '/dbfs/Users/jon.cheung@databricks.com/ray_xgboost_trainer/' \n",
    "# This is for stashing the cluster logs\n",
    "ray_logs_path = \"/dbfs/Users/jon.cheung@databricks.com/ray_collected_logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83361f2d-1dd3-421b-aad7-5028d3aef52c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.util.spark import setup_ray_cluster, shutdown_ray_cluster\n",
    "\n",
    "restart = True\n",
    "if restart is True:\n",
    "  try:\n",
    "    shutdown_ray_cluster()\n",
    "  except:\n",
    "    pass\n",
    "  try:\n",
    "    ray.shutdown()\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "# The below configuration mirrors my Spark worker cluster set up. Change this to match your cluster configuration. \n",
    "setup_ray_cluster(\n",
    "  min_worker_nodes=2,\n",
    "  max_worker_nodes=2,\n",
    "  num_cpus_worker_node=48,\n",
    "  num_gpus_worker_node=4,\n",
    "  num_cpus_head_node=48,\n",
    "  num_gpus_head_node=4,\n",
    "  collect_log_to_path=ray_logs_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4cb052f-751e-438f-b3cc-c9c5ddcb14e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "import ray.train\n",
    "from ray.train.xgboost import XGBoostTrainer, RayTrainReportCallback\n",
    "import os\n",
    "\n",
    "\n",
    "try: \n",
    "  ## Option 1 (PREFERRED): Build a Ray Dataset using a Databricks SQL Warehouse\n",
    "  # Insert your SQL warehouse ID here. I've queried my 100M row dataset using a Small t-shirt sized cluster.\n",
    "\n",
    "  # Ensure you've set the DATABRICKS_TOKEN so you can query using the warehouse compute\n",
    "  # Mine is commented out because I stashed it as an environment variable in the Spark config. \n",
    "  # os.environ['DATABRICKS_TOKEN'] = dbutils.secrets.get(scope = \"development\", key = \"jon_cheung_PAT\")\n",
    "  ds = ray.data.read_databricks_tables(\n",
    "    warehouse_id='2a72600bb68f00ee',\n",
    "    catalog=catalog,\n",
    "    schema=schema,\n",
    "    query=f'SELECT * FROM {table}',\n",
    "  )\n",
    "except: \n",
    "  ## Option 2: Build a Ray Dataset using a Parquet files\n",
    "  # If you have too many Ray nodes, you may not be able to create a Ray dataset using the warehouse method above because of rate limits. One back up solution is to create parquet files from the delta table and build a ray dataset from that. This is not the recommended route because, in essence, you are duplicating data.\n",
    "  parquet_path = f'/Volumes/{catalog}/{schema}/synthetic_data/{table}'\n",
    "  ds = ray.data.read_parquet(parquet_path)\n",
    "\n",
    "train_dataset, val_dataset = ds.train_test_split(test_size=0.25)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3060fa40-7b73-4524-8533-397e17f5c7fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import xgboost\n",
    "import ray.train\n",
    "from ray.train.xgboost import XGBoostTrainer, RayTrainReportCallback\n",
    "\n",
    "## Distributed training function per worker.\n",
    "# This should look very similar to a vanilla xgboost training.\n",
    "# In essence it's simply retrieving and training on a shard of the distributed dataset.\n",
    "def train_fn_per_worker(params: dict):\n",
    "    # Get dataset shards for this worker\n",
    "    train_shard = ray.train.get_dataset_shard(\"train\")\n",
    "    val_shard = ray.train.get_dataset_shard(\"val\")\n",
    "\n",
    "    # Convert shards to pandas DataFrames\n",
    "    train_df = train_shard.materialize().to_pandas()\n",
    "    val_df = val_shard.materialize().to_pandas()\n",
    "\n",
    "    train_X = train_df.drop(label, axis=1)\n",
    "    train_y = train_df[label]\n",
    "    val_X = val_df.drop(label, axis=1)\n",
    "    val_y = val_df[label]\n",
    "    \n",
    "    dtrain = xgboost.DMatrix(train_X, label=train_y)\n",
    "    deval = xgboost.DMatrix(val_X, label=val_y)\n",
    "\n",
    "    # Do distributed data-parallel training.\n",
    "    # Ray Train sets up the necessary coordinator processes and\n",
    "    # environment variables for workers to communicate with each other.\n",
    "    evals_results = {}\n",
    "    bst = xgboost.train(\n",
    "        params,\n",
    "        dtrain=dtrain,\n",
    "        evals=[(deval, \"validation\")],\n",
    "        num_boost_round=params['num_estimators'],\n",
    "        evals_result=evals_results,\n",
    "        # early_stopping_rounds=params['early_stopping_rounds'],\n",
    "        callbacks=[RayTrainReportCallback(metrics={params['eval_metric']: f\"validation-{params['eval_metric']}\"},\n",
    "                                          frequency=1)],\n",
    "    )\n",
    "    \n",
    "    # # Retrieve the evaluation metric values from the training process\n",
    "    final_eval_metric = evals_results['validation'][params['eval_metric']][-1]\n",
    "\n",
    "    # # Report evaluation metric to Ray Train\n",
    "    # ray.train.report({params['eval_metric']: final_eval_metric,\n",
    "    #                \"done\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2af42c58-c7c5-4b68-b13c-ee62eb956946",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from ray.tune.integration.ray_train import TuneReportCallback\n",
    "# The driver function is responsible for triggering distributed training,\n",
    "# collecting results, and coordinating workers. Here we use Ray's XGBoostTrainer\n",
    "def train_driver_fn(config: dict, train_dataset, val_dataset):\n",
    "    # Unpack run-level hyperparameters.\n",
    "    # Tune feeds in hyperparameters defined in the `param_space` below.\n",
    "    num_workers = config[\"num_workers\"]\n",
    "    use_gpu = config[\"use_gpu\"]\n",
    "    params = config['params']\n",
    "\n",
    "    trainer = XGBoostTrainer(\n",
    "      train_loop_per_worker=train_fn_per_worker,\n",
    "      train_loop_config=params,\n",
    "      # by default Ray uses 1 GPU and 1 CPU per worker if we don't specify resources_per_worker. \n",
    "      # Note that algorithms like XGBoost are multi-threaded so you can assign multiple CPUs per worker \n",
    "      # However, you cannot do the same for GPUs. Hence why we are doing DDP. \n",
    "      scaling_config=ray.train.ScalingConfig(num_workers=num_workers, \n",
    "                                             use_gpu=use_gpu),\n",
    "      datasets={\"train\": train_dataset, \"val\": val_dataset},\n",
    "      run_config=ray.train.RunConfig(storage_path=ray_xgboost_path)\n",
    "                                    # These parameters will be enabled below when we integrate this trainer with Ray Tune\n",
    "                                    #  callbacks=[TuneReportCallback()])\n",
    "                                    #  name=f\"train-trial_id={ray.tune.get_context().get_trial_id()}\")\n",
    "    )\n",
    "                                    \n",
    "    result = trainer.fit()\n",
    "    # propagate metrics back up\n",
    "    ray.tune.report({'mlogloss': result.metrics['mlogloss']})\n",
    "    # return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e55b33d8-dee0-41c8-9eec-4c42bdf32d45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # specify model hyper-parameters\n",
    "# # TODO: early_stopping_rounds will stop but causes an error in the RayTrainReportCallback. \n",
    "# # Perform one single run to ensure the driver function works\n",
    "# # I want to use 4 GPUs so I explicitly define 4 workers.\n",
    "# config = {\"num_workers\": 6,\n",
    "#           \"use_gpu\": True,\n",
    "#           \"params\":{\n",
    "#             \"objective\": \"multi:softmax\",\n",
    "#             'eval_metric': 'mlogloss', \n",
    "#             \"tree_method\": \"hist\",\n",
    "#             \"device\": \"cuda\",\n",
    "#             \"num_class\": 5,\n",
    "#             \"num_estimators\": 20\n",
    "#           }\n",
    "#           }\n",
    "\n",
    "\n",
    "# result = train_driver_fn(config=config,\n",
    "#                        train_dataset = train_dataset,\n",
    "#                        val_dataset = val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b570557e-327f-44b8-843d-728c57366cd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10ebc789-9d69-4a56-9c14-4bb6611bbf31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# with test.checkpoint.as_directory() as checkpoint_dir:\n",
    "#     model_path = os.path.join(checkpoint_dir, RayTrainReportCallback.CHECKPOINT_NAME)\n",
    "#     print(model_path)\n",
    "#     model = xgboost.Booster()\n",
    "#     model.load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4c23f41-5112-43ed-a668-7a284034eb3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Advanced: Ray Tune with Ray Train\n",
    "\n",
    "https://docs.ray.io/en/latest/train/user-guides/hyperparameter-optimization.html#hyperparameter-tuning-with-ray-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88750896-106b-4332-bee8-f930bc67f9f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "from ray.tune.tuner import Tuner\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "\n",
    "\n",
    "# Define resources per HPO trial and calculate max concurrent HPO trials\n",
    "num_gpu_workers_per_trial = 6\n",
    "num_samples = 2\n",
    "resources = ray.cluster_resources()\n",
    "total_cluster_gpus = resources.get(\"GPU\") \n",
    "max_concurrent_trials = int(total_cluster_gpus // num_gpu_workers_per_trial)\n",
    "\n",
    "\n",
    "# Define the hyperparameter search space.\n",
    "# XGB sample hyperparameter configs\n",
    "param_space = {\n",
    "    \"num_workers\": num_gpu_workers_per_trial,\n",
    "    \"use_gpu\": True,\n",
    "    \"params\":{\"objective\": \"multi:softmax\",\n",
    "              'eval_metric': 'mlogloss', \n",
    "              \"tree_method\": \"hist\",\n",
    "              \"device\": \"cuda\",\n",
    "              \"num_class\": num_labels,\n",
    "              \"learning_rate\": tune.uniform(0.01, 0.3),\n",
    "              \"num_estimators\": tune.randint(25, 50)}\n",
    "}\n",
    "\n",
    "# Set up search algorithm. Here we use Optuna and use the default the Bayesian sampler (i.e. TPES)\n",
    "# optuna = OptunaSearch(metric=\"mlogloss\", \n",
    "#                       mode=\"min\")\n",
    "\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    tune.with_parameters(train_driver_fn,\n",
    "                         train_dataset = train_dataset,\n",
    "                         val_dataset = val_dataset),\n",
    "    tune_config=tune.TuneConfig(num_samples=num_samples,\n",
    "                                # search_alg=optuna,\n",
    "                                max_concurrent_trials=max_concurrent_trials),\n",
    "    param_space=param_space,\n",
    "    )\n",
    "\n",
    "results = tuner.fit()\n",
    "\n",
    "results.get_best_result(metric=\"mlogloss\", \n",
    "                        mode=\"min\").config"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_c5986e9d-37a9-4446-a259-103b92642bc0",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3397375242589280,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "01_train_distributed_xgboost_GPUs",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
