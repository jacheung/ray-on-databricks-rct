{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e809f69b-44a6-4b71-a09a-979411e26579",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Distributed XGBoost with GPUs on Ray \n",
    "\n",
    "Ray provides a version of XGBoost to perform distributed data parallelism. With drop-in replacements of `xgboost` native classes, XGboost Ray allows you to leverage multi-node clusters to distribute your training. \n",
    "\n",
    "This demo uses a dataset created from `00-create-dataset` with 100M rows x 100 features columns x 1 target column (5 classes) for multi-class classification. This dataset is ~40GiB. \n",
    "\n",
    "`01a-train-with-GPUs` demonstrates in-core distributed training. This means you need enough VRAM for approximately ~2x the dataset size (i.e., 40GB dataset * 2 = 80GB VRAM). Using A10G (24GB each) we need approximately 3 A10s. \n",
    "\n",
    "`01b-train-with-GPUs-External-Memory` (this notebook) demonstrates out-of-core distributed training. The advantage here is that we can train with less VRAM than the full dataset size at the cost of needing to tune batch sizes (to maximize GPU and VRAM usage) and longer training times. \n",
    "\n",
    "\n",
    "#### Compute specifications to run this notebook\n",
    "```json\n",
    "{\n",
    "    \"num_workers\": 8,\n",
    "    \"cluster_name\": \"Multi-node MLR w/ GPUs\",\n",
    "    \"spark_version\": \"16.4.x-gpu-ml-scala2.13\",\n",
    "    \"spark_conf\": {\n",
    "        \"spark.task.resource.gpu.amount\": \"0\",\n",
    "        \"spark.executor.memory\": \"1g\"\n",
    "    },\n",
    "    \"aws_attributes\": {\n",
    "        \"first_on_demand\": 1,\n",
    "        \"availability\": \"SPOT_WITH_FALLBACK\",\n",
    "        \"zone_id\": \"auto\",\n",
    "        \"spot_bid_price_percent\": 100,\n",
    "        \"ebs_volume_count\": 0\n",
    "    },\n",
    "    \"node_type_id\": \"g5.8xlarge\",\n",
    "    \"driver_node_type_id\": \"g5.8xlarge\",\n",
    "    \"autotermination_minutes\": 60,\n",
    "    \"enable_elastic_disk\": false,\n",
    "    \"single_user_name\": \"jon.cheung@databricks.com\",\n",
    "    \"enable_local_disk_encryption\": false,\n",
    "    \"data_security_mode\": \"SINGLE_USER\",\n",
    "    \"runtime_engine\": \"STANDARD\",\n",
    "    \"assigned_principal\": \"user:jon.cheung@databricks.com\",\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45f9e867-2d65-4987-ac74-c84bc5b8a134",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -qU ray[all] xgboost rmm-cu12\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bdc7994-3c80-47bd-a10c-19038157cb75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "num_training_rows = 100_000_000\n",
    "num_training_columns = 100\n",
    "num_labels = 5\n",
    "catalog = \"main\"\n",
    "schema = \"ray_gtm_examples\"\n",
    "\n",
    "table = f\"synthetic_data_{num_training_rows}_rows_{num_training_columns}_columns_{num_labels}_labels\"\n",
    "label=\"target\"\n",
    "mlflow_experiment_name = f\"/Users/jon.cheung@databricks.com/ray_xgboost\"\n",
    "\n",
    "# If running in a multi-node cluster, this is where you\n",
    "# should configure the run's persistent storage that is accessible\n",
    "# across all worker nodes.\n",
    "ray_xgboost_path = '/dbfs/Users/jon.cheung@databricks.com/ray_xgboost/' \n",
    "# This is for stashing the cluster logs\n",
    "ray_logs_path = \"/dbfs/Users/jon.cheung@databricks.com/ray_collected_logs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83361f2d-1dd3-421b-aad7-5028d3aef52c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.util.spark import setup_ray_cluster, shutdown_ray_cluster\n",
    "import os\n",
    "\n",
    "restart = True\n",
    "if restart is True:\n",
    "  try:\n",
    "    shutdown_ray_cluster()\n",
    "  except:\n",
    "    pass\n",
    "  try:\n",
    "    ray.shutdown()\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "# Set the parameters here so mlflow works properly at the Ray head + worker nodes\n",
    "os.environ['DATABRICKS_HOST'] = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get()\n",
    "os.environ['DATABRICKS_TOKEN'] = dbutils.secrets.get(scope = \"development\", key = \"jon_cheung_PAT\")\n",
    "\n",
    "# The below configuration mirrors my Spark worker cluster set up. Change this to match your cluster configuration. \n",
    "setup_ray_cluster(\n",
    "  min_worker_nodes=8,\n",
    "  max_worker_nodes=8,\n",
    "  num_cpus_worker_node=32,\n",
    "  num_gpus_worker_node=1,\n",
    "  num_cpus_head_node=32,\n",
    "  num_gpus_head_node=1,\n",
    "  collect_log_to_path=ray_logs_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e1c7d89-a4dc-4e50-a60e-2e260d8d73fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import xgboost\n",
    "import rmm\n",
    "from rmm.allocators.cupy import rmm_cupy_allocator\n",
    "\n",
    "# It's important to use RMM for GPU-based external memory to improve performance.\n",
    "# If XGBoost is not built with RMM support, a warning will be raised.\n",
    "# We use the pool memory resource here for simplicity, you can also try the\n",
    "# `ArenaMemoryResource` for improved memory fragmentation handling.\n",
    "mr = rmm.mr.PoolMemoryResource(rmm.mr.CudaAsyncMemoryResource())\n",
    "rmm.mr.set_current_device_resource(mr)\n",
    "# Set the allocator for cupy as well.\n",
    "cp.cuda.set_allocator(rmm_cupy_allocator)\n",
    "\n",
    "class RayDataIter(xgboost.core.DataIter):\n",
    "    def __init__(self, ray_iterator: ray.data.DataIterator, label_col: str):\n",
    "        super().__init__()\n",
    "        self.label_col = label_col\n",
    "        self.iterator = ray_iterator\n",
    "        self._generator = None\n",
    "\n",
    "    def reset(self):\n",
    "        self._generator = iter(self.iterator)\n",
    "\n",
    "    def next(self, input_data):\n",
    "        try:\n",
    "            batch = next(self._generator)\n",
    "        except StopIteration:\n",
    "            return False\n",
    "        \n",
    "        y = cp.asarray(batch[self.label_col])\n",
    "        X = cp.column_stack([cp.asarray(batch[col]) for col in batch if col != self.label_col])\n",
    "\n",
    "        input_data(data=X, label=y)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4cb052f-751e-438f-b3cc-c9c5ddcb14e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "import ray.train\n",
    "from ray.train.xgboost import XGBoostTrainer, RayTrainReportCallback\n",
    "import os\n",
    "\n",
    "\n",
    "try: \n",
    "  ## Option 1 (PREFERRED): Build a Ray Dataset using a Databricks SQL Warehouse\n",
    "  # Insert your SQL warehouse ID here. I've queried my 100M row dataset using a Small t-shirt sized cluster.\n",
    "\n",
    "  # Ensure you've set the DATABRICKS_TOKEN so you can query using the warehouse compute\n",
    "  ds = ray.data.read_databricks_tables(\n",
    "    warehouse_id='2a72600bb68f00ee',\n",
    "    catalog=catalog,\n",
    "    schema=schema,\n",
    "    query=f'SELECT * FROM {table}',\n",
    "  )\n",
    "  # Testing out read from UC. However, this requires turning off deletion vectors\n",
    "  # ds = ray.data.read_unity_catalog(f'{catalog}.{schema}.{table}',\n",
    "  #                                url='https://e2-demo-field-eng.cloud.databricks.com/',\n",
    "  #                                token=os.environ['DATABRICKS_TOKEN'],\n",
    "  #                                region='us-west-2',\n",
    "  #                                reader_kwargs={\"override_num_blocks\": 1000})\n",
    "  print('read directly from UC')\n",
    "except: \n",
    "  ## Option 2: Build a Ray Dataset using a Parquet files\n",
    "  # If you have too many Ray nodes, you may not be able to create a Ray dataset using the warehouse method above because of rate limits. One back up solution is to create parquet files from the delta table and build a ray dataset from that. This is not the recommended route because, in essence, you are duplicating data.\n",
    "  parquet_path = f'/Volumes/{catalog}/{schema}/synthetic_data/{table}'\n",
    "  ds = ray.data.read_parquet(parquet_path)\n",
    "  print('read directly from parquet')\n",
    "\n",
    "train_dataset, val_dataset = ds.train_test_split(test_size=0.25)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3060fa40-7b73-4524-8533-397e17f5c7fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import xgboost\n",
    "import ray.train\n",
    "from ray.train.xgboost import XGBoostTrainer, RayTrainReportCallback\n",
    "\n",
    "\n",
    "def train_fn_per_worker(params: dict):\n",
    "    \"\"\"\n",
    "    Trains an XGBoost model on a shard of the distributed dataset assigned to this worker.\n",
    "\n",
    "    This should look very similar to a vanilla XGboost training.\n",
    "\n",
    "    This function is designed to be executed by individual Ray Train workers.\n",
    "    It retrieves the training and validation data shards, converts them to DMatrix format,\n",
    "    and performs a portion of the distributed XGBoost training. Ray Train handles\n",
    "    the inter-worker communication.\n",
    "\n",
    "    Args:\n",
    "        params (dict): A dictionary of XGBoost training parameters, including\n",
    "                       'num_estimators', 'eval_metric', and potentially other\n",
    "                       XGBoost-specific parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get dataset shards for this worker\n",
    "    train_shard = ray.train.get_dataset_shard(\"train\")\n",
    "    val_shard = ray.train.get_dataset_shard(\"val\")\n",
    "\n",
    "    ### INSERTED\n",
    "    iterator = train_shard.iter_batches(batch_format=\"numpy\", batch_size=256*16_384, prefetch_batches=2)\n",
    "    streaming_iter = RayDataIter(iterator, label_col=label)\n",
    "\n",
    "    val_iterator = val_shard.iter_batches(batch_format=\"numpy\", batch_size=256*16_384, prefetch_batches=2)\n",
    "    streaming_val_iter = RayDataIter(val_iterator, label_col=label)\n",
    "    with xgboost.config_context(use_rmm=True):\n",
    "        # External Quantile DMatrix (streams data, minimal memory usage, GPU-optimized)\n",
    "        qdm_train = xgboost.ExtMemQuantileDMatrix(streaming_iter)\n",
    "        # qdm_val = xgboost.DMatrix(streaming_val_iter, ref=qdm_train)\n",
    "        qdm_val = xgboost.ExtMemQuantileDMatrix(streaming_val_iter, ref=qdm_train)\n",
    "\n",
    "        # Do distributed data-parallel training.\n",
    "        # Ray Train sets up the necessary coordinator processes and\n",
    "        # environment variables for workers to communicate with each other.\n",
    "        evals_results = {}\n",
    "        bst = xgboost.train(\n",
    "            params,\n",
    "            dtrain=qdm_train,\n",
    "            evals=[(qdm_val, \"validation\")],\n",
    "            num_boost_round=params['num_estimators'],\n",
    "            evals_result=evals_results,\n",
    "            # early_stopping_rounds=params['early_stopping_rounds'],\n",
    "            callbacks=[RayTrainReportCallback(metrics={params['eval_metric']: f\"validation-{params['eval_metric']}\"},\n",
    "                                            frequency=1)],\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2af42c58-c7c5-4b68-b13c-ee62eb956946",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def train_driver_fn(config: dict, train_dataset, val_dataset):\n",
    "    \"\"\"\n",
    "    Drives the distributed XGBoost training process using Ray Train.\n",
    "\n",
    "    This function sets up the XGBoostTrainer, configures scaling (number of workers, GPU usage,\n",
    "    and resources per worker), and initiates the distributed training by calling `trainer.fit()`.\n",
    "    It also propagates metrics back to Ray Tune if integrated.\n",
    "\n",
    "    Args:\n",
    "        config (dict): A dictionary containing run-level hyperparameters such as\n",
    "                       'num_workers', 'use_gpu', and a nested 'params' dictionary\n",
    "                       for XGBoost training parameters.\n",
    "        train_dataset: The Ray Dataset for training.\n",
    "        val_dataset: The Ray Dataset for validation.\n",
    "\n",
    "    Returns:\n",
    "        None: The function reports metrics to Ray Tune but does not explicitly return a value.\n",
    "              The trained model artifact is typically handled by Ray Train's checkpointing\n",
    "              or by the `train_fn_per_worker` if saved directly.\n",
    "    \"\"\"\n",
    "    # Unpack run-level hyperparameters.\n",
    "    num_workers = config[\"num_workers\"]\n",
    "    use_gpu = config[\"use_gpu\"]\n",
    "    params = config['params']\n",
    "\n",
    "    # Initialize the XGBoostTrainer, which orchestrates the distributed training using Ray.\n",
    "    trainer = XGBoostTrainer(\n",
    "      train_loop_per_worker=train_fn_per_worker, # The function to be executed on each worker\n",
    "      train_loop_config=params,\n",
    "      # By default Ray uses 1 GPU and 1 CPU per worker if resources_per_worker is not specified.\n",
    "      # XGBoost is multi-threaded, so multiple CPUs can be assigned per worker, but not GPUs.\n",
    "      scaling_config=ray.train.ScalingConfig(num_workers=num_workers, \n",
    "                                             use_gpu=use_gpu),\n",
    "      datasets={\"train\": train_dataset, \"val\": val_dataset},  # Ray Datasets to be used by the trainer + workers\n",
    "      run_config=ray.train.RunConfig(storage_path=ray_xgboost_path,                                  \n",
    "                                    #  name=f\"train-trial_id={ray.tune.get_context().get_trial_id()}\")\n",
    "      )\n",
    "    )\n",
    "                                    \n",
    "    result = trainer.fit()\n",
    "    \n",
    "    # Propagate metrics back up for Ray Tune. \n",
    "    # Ensure 'mlogloss' is the correct metric key based on your eval_metric and results.\n",
    "    ray.tune.report({params['eval_metric']: result.metrics['mlogloss']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10ebc789-9d69-4a56-9c14-4bb6611bbf31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# with test.checkpoint.as_directory() as checkpoint_dir:\n",
    "#     model_path = os.path.join(checkpoint_dir, RayTrainReportCallback.CHECKPOINT_NAME)\n",
    "#     print(model_path)\n",
    "#     model = xgboost.Booster()\n",
    "#     model.load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4c23f41-5112-43ed-a668-7a284034eb3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Ray Tune with Ray Train and Mlflow Integration\n",
    "\n",
    "https://docs.ray.io/en/latest/train/user-guides/hyperparameter-optimization.html#hyperparameter-tuning-with-ray-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88750896-106b-4332-bee8-f930bc67f9f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "from ray.tune.tuner import Tuner\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from ray.air.integrations.mlflow import MLflowLoggerCallback\n",
    "\n",
    "\n",
    "# Define resources per HPO trial and calculate max concurrent HPO trials\n",
    "num_gpu_workers_per_trial = 1\n",
    "num_hpo_trials = 8\n",
    "resources = ray.cluster_resources()\n",
    "total_cluster_gpus = resources.get(\"GPU\") \n",
    "max_concurrent_trials = int(total_cluster_gpus // num_gpu_workers_per_trial)\n",
    "\n",
    "\n",
    "# Define the hyperparameter search space.\n",
    "# XGB sample hyperparameter configs\n",
    "param_space = {\n",
    "    \"num_workers\": num_gpu_workers_per_trial,\n",
    "    \"use_gpu\": True,\n",
    "    \"params\":{\"objective\": \"multi:softmax\",\n",
    "              'eval_metric': 'mlogloss', \n",
    "              \"tree_method\": \"hist\",\n",
    "              \"device\": \"cuda\",\n",
    "              \"num_class\": num_labels,\n",
    "              \"learning_rate\": tune.uniform(0.01, 0.3),\n",
    "              \"num_estimators\": tune.randint(25, 50),\n",
    "              'sampling_method': 'gradient_based',}\n",
    "}\n",
    "\n",
    "# # Set up search algorithm. Here we use Optuna and use the default the Bayesian sampler (i.e. TPES)\n",
    "optuna = OptunaSearch(metric=param_space['params']['eval_metric'], \n",
    "                      mode=\"min\")\n",
    "\n",
    "# Set up Tuner job and run.\n",
    "tuner = tune.Tuner(\n",
    "    tune.with_parameters(train_driver_fn,\n",
    "                         train_dataset = train_dataset,\n",
    "                         val_dataset = val_dataset),\n",
    "    run_config=tune.RunConfig(name='mlflow',\n",
    "                              callbacks=[MLflowLoggerCallback(\n",
    "                                  experiment_name=mlflow_experiment_name,\n",
    "                                  save_artifact=True,\n",
    "                                  log_params_on_trial_end=True)]\n",
    "                              ),\n",
    "    tune_config=tune.TuneConfig(num_samples=num_hpo_trials,\n",
    "                                max_concurrent_trials=max_concurrent_trials,\n",
    "                                search_alg=optuna,\n",
    "                                ),\n",
    "    param_space=param_space,\n",
    "\n",
    "    )\n",
    "\n",
    "results = tuner.fit()\n",
    "\n",
    "best_params = results.get_best_result(metric=param_space['params']['eval_metric'], \n",
    "                        mode=\"min\").config\n",
    "\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74e36c66-b6ae-40a1-ad45-8a2f0c40db5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_c5986e9d-37a9-4446-a259-103b92642bc0",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3397375242701867,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "01b-train-with-GPUs-External-Memory",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
