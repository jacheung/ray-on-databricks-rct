{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0c63aaf-35c5-45c3-a031-89cb5bf4d898",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Parallelized Prophet Modeling with Ray\n",
    "\n",
    "Prophet is a simple, yet powerful, additive forecasting model. To the former, it's implementation is intuitive and requires editing a few parameters and, to the latter, it provides an algorithmically efficient way to identify time-related patterns in the data. These two aspects make Prophet an ideal starting, and possibly end, point for a forecasting model. \n",
    "\n",
    "However, in real-world production use-cases we must overcome scaling challenges in model training and inference. Specifically, in retail use-cases we'd like to generate forecasting models for every combination of store x SKU. This can lead to 100K+ models. Furthermore, business demands may require all these models be trained overnight on a weekly basis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19042c4f-f4d5-4e17-8030-cdce13849787",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -qU mlflow ray[default]==2.44.1 ray[data]==2.44.1 delta-sharing\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "648b7a92-5225-4115-8521-df6cc892bbae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = \"main\"\n",
    "schema = \"ray_gtm_examples\"\n",
    "table = \"data_synthetic_timeseries_10000_groups\"\n",
    "write_table = \"prophet_model_directory\"\n",
    "label=\"y\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "619cb71f-f3f8-4b48-87cc-9c4050665b5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Optional: Generate a massive time-series dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ba0d28a-03b7-428d-ab0e-bc7babf504da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# default is 10k groups x 50k datapoints per group. Edit within the 00_generate_timeseries_data notebook\n",
    "# This may take 15 mins or so to generate all the data and save. \n",
    "%run ./00_generate_timeseries_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccf949cc-893d-4248-bc63-e49bf38b78c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Synthetic data generation \n",
    "import pandas as pd\n",
    "\n",
    "if not spark.catalog.tableExists(f\"{catalog}.{schema}.{table}\"): \n",
    "  # Create table for features\n",
    "  id_sdf.write.mode('overwrite').saveAsTable(f\"{catalog}.{schema}.{table}\")\n",
    "  print(f\"... OK!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfc4378b-3c77-436d-9955-dc202085a6bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ray Data with `map_groups`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14052039-5490-4495-9dbf-35a4319b8f90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.util.spark import setup_ray_cluster, shutdown_ray_cluster\n",
    "\n",
    "restart = True\n",
    "if restart is True:\n",
    "  try:\n",
    "    shutdown_ray_cluster()\n",
    "  except:\n",
    "    pass\n",
    "  try:\n",
    "    ray.shutdown()\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "# The below configuration mirrors my Spark worker cluster set up. Change this to match your cluster configuration. \n",
    "setup_ray_cluster(\n",
    "  min_worker_nodes=6,\n",
    "  max_worker_nodes=6,\n",
    "  num_cpus_worker_node=16,\n",
    "  num_gpus_worker_node=0,\n",
    "  collect_log_to_path=\"/dbfs/Users/jon.cheung@databricks.com/ray_collected_logs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95c8549b-83a6-40cd-aec6-b329c19a1b2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from prophet import Prophet\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import os\n",
    "from mlflow.utils.databricks_utils import get_databricks_env_vars\n",
    "from ray.data import from_spark\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "def train_and_inference_prophet(grouped_data:pd.DataFrame):\n",
    "        # Create nested child runs named after the group\n",
    "        group_name = grouped_data.loc[0,'group_name']\n",
    "\n",
    "        # fit the model and generate forecasts\n",
    "        m = Prophet(daily_seasonality=True)\n",
    "        m.fit(grouped_data)\n",
    "\n",
    "        base_dir = f'/Volumes/{catalog}/{schema}/prophet_binaries/'\n",
    "        file_name = f'{group_name}_prophet.pkl'\n",
    "        with open(base_dir+file_name, 'wb') as file:\n",
    "                pickle.dump(m, file)\n",
    "\n",
    "        # Write an output dataframe\n",
    "        to_write = pd.DataFrame({'group_name': [group_name],\n",
    "                                'model_binary_directory': [base_dir+file_name],\n",
    "                                'algorithm': ['prophet'],\n",
    "                                'creation_time': [str(datetime.now())]})\n",
    "\n",
    "        return to_write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88fd8775-6bf3-4053-b16e-7210714c4851",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create volume for Ray Data to store the data shards. This intermediate volume is also used when writing back to a Spark table from Ray Data. \n",
    "os.environ['RAY_UC_VOLUMES_FUSE_TEMP_DIR'] = f'/Volumes/{catalog}/{schema}/ray_data_tmp_dir'\n",
    "\n",
    "# Convert Spark data to Ray data. Note that this is an in-memory operation.\n",
    "ray_data = from_spark(spark.read.table(f\"{catalog}.{schema}.{table}\"), \n",
    "                      use_spark_chunk_api=False)\n",
    "\n",
    "\n",
    "# TODO # Read in Ray Data from Delta Sharing for memory efficient load compared to from_spark (i.e. no need to do in-memory Spark -> Ray Data)\n",
    "# profile_file = \"config.share\"\n",
    "# SHARE_NAME = 'internal-ray-share'\n",
    "# SHARE_SCHEMA = 'ray_gtm_examples'\n",
    "# SHARE_TABLE = 'data_synthetic_timeseries_100_groups'\n",
    "# table_url = f\"{profile_file}#{SHARE_NAME}.{SHARE_SCHEMA}.{SHARE_TABLE}\"\n",
    "\n",
    "# ray_data = ray.data.read_delta_sharing_tables(\n",
    "#     url=table_url\n",
    "# )\n",
    "\n",
    "# Start the map_groups process.                       \n",
    "grouped = ray_data.groupby(\"group_name\")\n",
    "results = grouped.map_groups(train_and_inference_prophet, \n",
    "                             num_cpus=1)\n",
    "\n",
    "# # write grouped results to a Delta table\n",
    "ray.data.Dataset.write_databricks_table(results, \n",
    "                                        f\"{catalog}.{schema}.{write_table}\",\n",
    "                                         mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e38ac2ca-df10-41a9-bbc7-66592a4bb3f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "experiment_name = '/Users/jon.cheung@databricks.com/ray_prophet_map_batches'\n",
    "\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.catalog import *\n",
    "\n",
    "w = WorkspaceClient(host='https://xxx.databricks.com', token='xxx')\n",
    "\n",
    "class ProphetInferenceRouter(mlflow.pyfunc.PythonModel):\n",
    "  def __init__(self, catalog, schema, table):\n",
    "    self.model_table = f\"{catalog}.{schema}.{table}\"\n",
    "\n",
    "  def predict(self, identifier, horizon):\n",
    "    \n",
    "    \n",
    "    loaded = pickle.loads(model_binary)\n",
    "    future = loaded.make_future_dataframe(periods=horizon)\n",
    "    forecast = loaded.predict(future)\n",
    "\n",
    "    \n",
    "  "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1436459421794120,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "02_*scaled*_parallelize_prophet_forecasting",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
