{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0c63aaf-35c5-45c3-a031-89cb5bf4d898",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Parallelized Prophet Modeling with Ray\n",
    "\n",
    "Prophet is a simple, yet powerful, additive forecasting model. To the former, it's implementation is intuitive and requires editing a few parameters and, to the latter, it provides an algorithmically efficient way to identify time-related patterns in the data. These two aspects make Prophet an ideal starting, and possibly end, point for a forecasting model. \n",
    "\n",
    "However, in real-world production use-cases we must overcome scaling challenges in model training and inference. Specifically, in retail use-cases we'd like to generate forecasting models for every combination of store x SKU. This can lead to 100K+ models. Furthermore, business demands may require all these models be trained overnight on a weekly basis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19042c4f-f4d5-4e17-8030-cdce13849787",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -qU mlflow ray[default]==2.44.1 ray[data]==2.44.1\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "648b7a92-5225-4115-8521-df6cc892bbae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = \"main\"\n",
    "schema = \"ray_gtm_examples\"\n",
    "table = \"data_synthetic_timeseries_10000_groups\"\n",
    "write_table = \"prophet_forecasts\"\n",
    "label=\"y\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "619cb71f-f3f8-4b48-87cc-9c4050665b5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Optional: Generate a massive time-series dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ba0d28a-03b7-428d-ab0e-bc7babf504da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# default is 10k groups x 50k datapoints per group. Edit within the 00_generate_timeseries_data notebook\n",
    "# This may take 15 mins or so to generate all the data and save. \n",
    "%run ./00_generate_timeseries_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccf949cc-893d-4248-bc63-e49bf38b78c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Synthetic data generation \n",
    "import pandas as pd\n",
    "\n",
    "if not spark.catalog.tableExists(f\"{catalog}.{schema}.{table}\"): \n",
    "  # Create table for features\n",
    "  id_sdf.write.mode('overwrite').saveAsTable(f\"{catalog}.{schema}.{table}\")\n",
    "  print(f\"... OK!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfc4378b-3c77-436d-9955-dc202085a6bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ray Data with `map_groups`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14052039-5490-4495-9dbf-35a4319b8f90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.util.spark import setup_ray_cluster, shutdown_ray_cluster\n",
    "\n",
    "restart = True\n",
    "if restart is True:\n",
    "  try:\n",
    "    shutdown_ray_cluster()\n",
    "  except:\n",
    "    pass\n",
    "  try:\n",
    "    ray.shutdown()\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "# The below configuration mirrors my Spark worker cluster set up. Change this to match your cluster configuration. \n",
    "setup_ray_cluster(\n",
    "  min_worker_nodes=6,\n",
    "  max_worker_nodes=6,\n",
    "  num_cpus_worker_node=16,\n",
    "  num_gpus_worker_node=0,\n",
    "  collect_log_to_path=\"/dbfs/Users/jon.cheung@databricks.com/ray_collected_logs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95c8549b-83a6-40cd-aec6-b329c19a1b2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from prophet import Prophet\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import os\n",
    "from mlflow.utils.databricks_utils import get_databricks_env_vars\n",
    "from ray.data import from_spark\n",
    "import os\n",
    "\n",
    "# Create volume for Ray Data to store the data shards. This intermediate volume is also used when writing back to a Spark table from Ray Data. \n",
    "os.environ['RAY_UC_VOLUMES_FUSE_TEMP_DIR'] = f'/Volumes/{catalog}/{schema}/ray_data_tmp_dir'\n",
    "\n",
    "experiment_name = '/Users/jon.cheung@databricks.com/ray_prophet_map_batches'\n",
    "mlflow.set_experiment(experiment_name)\n",
    "mlflow_db_creds = get_databricks_env_vars(\"databricks\")\n",
    "\n",
    "def train_and_inference_prophet(grouped_data:pd.DataFrame, \n",
    "                                horizon:int,\n",
    "                                parent_run_id: str\n",
    "                                ):\n",
    "        # Set mlflow credentials and active MLflow experiment within each Ray actor\n",
    "        os.environ.update(mlflow_db_creds)\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "\n",
    "        # Create nested child runs named after the group\n",
    "        group_name = grouped_data.loc[0,'group_name']\n",
    "        with mlflow.start_run(run_name = f\"{group_name}\",\n",
    "                              parent_run_id=parent_run_id,\n",
    "                              nested=True):\n",
    "\n",
    "          # fit the model and generate forecasts\n",
    "          m = Prophet(daily_seasonality=True)\n",
    "          m.fit(grouped_data)\n",
    "          future = m.make_future_dataframe(periods=horizon)\n",
    "          forecast = m.predict(future)\n",
    "          \n",
    "          # edit forecasts Dataframe for delta table logging\n",
    "          to_write = forecast.iloc[-horizon:]\n",
    "          to_write['ds'] = to_write['ds'].astype(str)\n",
    "          to_write['group_name'] = group_name\n",
    "\n",
    "          # mlflow logging\n",
    "          dataset = mlflow.data.from_pandas(grouped_data)\n",
    "          mlflow.log_input(dataset)\n",
    "          mlflow.prophet.log_model(pr_model=m,\n",
    "                                   artifact_path=\"prophet_model\")\n",
    "\n",
    "        return to_write\n",
    "\n",
    "# Convert Spark data to Ray data. Note that this is an in-memory operation.\n",
    "ray_data = from_spark(spark.read.table(f\"{catalog}.{schema}.{table}\"), \n",
    "                      use_spark_chunk_api=False)\n",
    "\n",
    "# Create an mlflow run and start the map_groups process.                       \n",
    "with mlflow.start_run(run_name=\"prophet_models_250403\") as parent_run: \n",
    "  grouped = ray_data.groupby(\"group_name\")\n",
    "  results = grouped.map_groups(train_and_inference_prophet, \n",
    "              num_cpus=1,\n",
    "              fn_kwargs={\"horizon\": 14,\n",
    "                         \"parent_run_id\": parent_run.info.run_id})\n",
    "\n",
    "# write grouped results to a Delta table\n",
    "ray.data.Dataset.write_databricks_table(results, \n",
    "                                        f\"{catalog}.{schema}.{write_table}\",\n",
    "                                         mode='overwrite')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d1e0139-a0e3-45e1-821c-0a5698aae5d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "01_parallelize_prophet_forecasting",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
