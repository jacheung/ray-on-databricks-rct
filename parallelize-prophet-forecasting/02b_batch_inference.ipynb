{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7da784ba-c465-4a8b-9a3f-f5f9ec9f5cbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install delta-sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d8b6f4d-5913-43b6-97cc-236ba35ad59d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO # Read in Ray Data from Delta Sharing for memory efficient load compared to from_spark (i.e. no need to do in-memory Spark -> Ray Data)\n",
    "profile_file = \"config.share\"\n",
    "SHARE_NAME = 'internal-ray-share'\n",
    "SHARE_SCHEMA = 'ray_gtm_examples'\n",
    "SHARE_VOLUME = 'prophet_binaries'\n",
    "table_url = f\"{profile_file}#{SHARE_NAME}.{SHARE_SCHEMA}.{SHARE_VOLUME}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e42cdfbe-a9c1-40f2-809b-2ef92c046470",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta_sharing import SharingClient\n",
    "\n",
    "client = SharingClient(profile_file)\n",
    "\n",
    "df = spark.read.format(\"delta\").load(f\"delta.{SHARE_NAME}.{SHARE_SCHEMA}.{SHARE_VOLUME}\")\n",
    "\n",
    "# Display the data\n",
    "df.show()\n",
    "\n",
    "# # List all files in the specific share volume\n",
    "# files = client.list_files_in_table(SHARE_NAME, SHARE_SCHEMA, SHARE_VOLUME)\n",
    "# display(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c57a829-ee6b-4925-b774-840545ec13ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "experiment_name = '/Users/jon.cheung@databricks.com/ray_prophet_map_batches'\n",
    "\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.catalog import *\n",
    "\n",
    "\n",
    "catalog            = 'main'\n",
    "schema             = 'ray_gtm_examples'\n",
    "volume             = 'prophet_binaries'\n",
    "volume_path        = f\"/Volumes/{catalog}/{schema}/{volume}\" \n",
    "\n",
    "\n",
    "class ProphetInferenceRouter(mlflow.pyfunc.PythonModel):\n",
    "  def __init__(self, volume_path):\n",
    "    self.workspace_model_binaries = volume_path\n",
    "\n",
    "  def load_context(self, context): \n",
    "    self.router = pd.read_parquet(context['model_router'])\n",
    "    \n",
    "  def _connect_to_workspace_client():\n",
    "    w = WorkspaceClient(host='https://e2-demo-field-eng.cloud.databricks.com/',\n",
    "                        token='xxx')\n",
    "\n",
    "\n",
    "  def predict(self, model_input):\n",
    "    \n",
    "    model_path = self.router.loc[self.router['group_name'] == model_input['identifier'], 'model_path'].values[0]\n",
    "    model = pickle.loads(model_path)\n",
    "    future = model.make_future_dataframe(periods=horizon)\n",
    "    forecast = model.predict(future)\n",
    "\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57b100c0-46a8-4fca-8ea5-ab782a5884f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    "  \"name\": \"endpoint-name\",\n",
    "  \"config\": {\n",
    "    \"served_entities\": [\n",
    "      {\n",
    "        \"entity_name\": \"model-name\",\n",
    "        \"entity_version\": \"1\",\n",
    "        \"workload_size\": \"Small\",\n",
    "        \"scale_to_zero_enabled\": \"true\",\n",
    "        \"environment_vars\": {\n",
    "          \"OPENAI_API_KEY\": \"{{secrets/my_secret_scope/my_secret_key}}\"\n",
    "        }\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "02b_batch_inference",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
