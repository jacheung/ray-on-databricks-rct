{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73da66b1-6e80-46b2-829d-afac2e200abd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Parallelized Bayesian Hyperparameter Tuning for XGBoost with Ray \n",
    "\n",
    "Boosting algorithms, like XGboost, offers a simple, yet powerful, model to solve many regression and classification problems. However, they are prone to overfitting and require hyperparameter tuning with validation datasets to ensure they can be generalized to the real-world problems they are meant to solve. When it comes to hyperparameter tuning, traditional grid-search is inefficient (i.e. unnecessarily time-consuming). It offers little benefit over more efficient methods like Bayesian search, especially when the search space is large. To double-click on this, Bayesian search balances exploration and exploitation. It explores the search space and uses this as a prior to determine which area to search more in-depth for later trials. \n",
    "\n",
    "This notebook outlines two powerful additions to XGBoost to improve (i.e. make more efficient) hyperparameter search. They are:\n",
    "1. Ray for parallelized search -- specifically single-node multi-GPU\n",
    "2. Optuna for Bayesian search\n",
    "\n",
    "This notebook is also specific to using a single-node multi-GPU cluster. I'd suggest benchmarking on one trial to see if parallelizing with GPUs is worth it compared to CPUs. For example, Im currently running this on a node with 4 GPUs and 48 CPUs. Barring any memory constraints, unless a GPU is 12 times more efficient than CPU it's more efficent to use CPUs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06d85b6a-e2a8-4f86-93ae-241fdc7ddd33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install optuna ray[data]==2.37.0 ray[train]==2.37.0 ray[tune]==2.37.0\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b707d7fb-a85a-4203-bf86-a9de40235c9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = \"main\"\n",
    "schema = \"ray_gtm_examples\"\n",
    "num_labels=5\n",
    "num_training_rows = 25_000_000\n",
    "num_training_columns = 100\n",
    "\n",
    "if num_labels > 2:\n",
    "  table_path = f\"synthetic_data_{num_training_rows}_rows_{num_training_columns}_columns_{num_labels}_labels\"\n",
    "else:\n",
    "  table_path = f\"synthetic_data_{num_training_rows}_rows_{num_training_columns}_columns\"\n",
    "  \n",
    "parquet_path = f\"/Volumes/{catalog}/{schema}/synthetic_data/{table_path}\"\n",
    "print(f\"Parquet path: {parquet_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "faea8444-9297-4024-bd08-d9a8abe96015",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.util.spark import setup_ray_cluster, shutdown_ray_cluster\n",
    "import os\n",
    "\n",
    "restart = True\n",
    "if restart is True:\n",
    "  try:\n",
    "    shutdown_ray_cluster()\n",
    "  except:\n",
    "    pass\n",
    "  try:\n",
    "    ray.shutdown()\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "\n",
    "context = ray.init(\n",
    "  include_dashboard=True,\n",
    "  dashboard_host=\"0.0.0.0\",\n",
    "  dashboard_port=9999\n",
    "  )\n",
    "  \n",
    "\n",
    "def get_dashboard_url(spark=spark, dbutils=dbutils, dashboard_port='9999'):  \n",
    "  base_url='https://' + spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
    "  workspace_id=spark.conf.get(\"spark.databricks.clusterUsageTags.orgId\")\n",
    "  cluster_id=spark.conf.get(\"spark.databricks.clusterUsageTags.clusterId\")\n",
    "\n",
    "  pathname_prefix='/driver-proxy/o/' + workspace_id + '/' + cluster_id + '/' + dashboard_port+\"/\" \n",
    "  apitoken = dbutils.notebook().entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "  dashboard_url=base_url + pathname_prefix  \n",
    "\n",
    "  return dashboard_url\n",
    "  \n",
    "print(get_dashboard_url())\n",
    "# Ray dashboard at URL like: https://dbc-dp-1444828305810485.cloud.databricks.com/driver-proxy/o/1444828305810485/0325-215413-crpqybob/9999/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f22eea1-be50-4f22-8fcc-52cc79d48639",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import xgboost as xgb\n",
    "# import numpy as np\n",
    "# from sklearn.datasets import make_classification\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # 1. Set up sample data\n",
    "# X, y = make_classification(n_samples=1_000_000, n_features=20, random_state=42)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Convert to DMatrix format\n",
    "# dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "# dtest = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb110039-b400-4537-b0b8-54e6634da37e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import ray\n",
    "from ray.data import Dataset\n",
    "\n",
    "def prepare_data() -> Tuple[Dataset, Dataset, Dataset]:\n",
    "    \n",
    "    dataset = ray.data.read_parquet(parquet_path)\n",
    "    train_dataset, valid_dataset = dataset.train_test_split(test_size=0.3, seed=42)\n",
    "\n",
    "    return train_dataset, valid_dataset\n",
    "  \n",
    "train_dataset, valid_dataset = prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bbb66ab-0faa-470a-8b88-69c448ad9e2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import xgboost as xgb\n",
    "\n",
    "devices = ['cpu', 'cuda']\n",
    "run_time = {}\n",
    "\n",
    "for device in devices:\n",
    "  params = {\n",
    "      \"objective\": \"multi:softmax\",\n",
    "      'eval_metric': 'mlogloss', \n",
    "      \"num_class\": num_labels, \n",
    "      'eta': 0.1,\n",
    "      'max_depth': 6,\n",
    "      'tree_method': 'hist',    # Use GPU acceleration\n",
    "      'device': device,         # Primary GPU ID\n",
    "  }\n",
    "\n",
    "  # Convert to DMatrix format\n",
    "  train_df, test_df = train_dataset.to_pandas(), valid_dataset.to_pandas()\n",
    "  X_train, y_train = train_df.drop(\"target\", axis=1), train_df[\"target\"]\n",
    "  X_test, y_test = test_df.drop(\"target\", axis=1), test_df[\"target\"]\n",
    "  dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "  dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "  # 3. Train the model with one GPU\n",
    "  start_time = time.time()\n",
    "  evals_result = {}\n",
    "  model = xgb.train(\n",
    "      params,\n",
    "      dtrain,\n",
    "      num_boost_round=1000,\n",
    "      evals=[(dtrain, 'train'), (dtest, 'test')],\n",
    "      evals_result=evals_result,\n",
    "      verbose_eval=10\n",
    "  )\n",
    "\n",
    "  final_time = (time.time() - start_time)\n",
    "  run_time[device] = final_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44837924-cf8f-415e-b620-576cc9b6b12e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "run_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f17fb694-3280-49c2-9e5c-9ce093f26625",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# https://optuna.readthedocs.io/en/stable/faq.html#how-can-i-use-two-gpus-for-evaluating-two-trials-simultaneously\n",
    "# create one main.py \n",
    "    # params: train dataset, test dataset, target_column, optuna_study_name, CUDA_VISIBLE_DEVICES \n",
    "# run each main.py with a different CUDA_VISIBLE_DEVICES\n",
    "# Multi-task job. One task per GPU available on single-node.\n",
    "\n",
    "# or use ray in a single notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c52e415-f77b-4efc-b8de-e4b4e5f2d757",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import mlflow\n",
    "from mlflow.utils.databricks_utils import get_databricks_env_vars\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from ray import train, tune\n",
    "from ray.air.integrations.mlflow import MLflowLoggerCallback\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "import xgboost as xgb\n",
    "\n",
    "# Set the number of trials to run\n",
    "num_samples = 12\n",
    "\n",
    "# Set mlflow experiment name\n",
    "experiment_name = '/Users/jon.cheung@databricks.com/ray-xgb-gpu'\n",
    "mlflow.set_experiment(experiment_name)\n",
    "mlflow_db_creds = get_databricks_env_vars(\"databricks\")\n",
    "\n",
    "# Define a training function to parallelize\n",
    "def train_classifier(config: dict,\n",
    "                    experiment_name: str,\n",
    "                    parent_run_id: str,\n",
    "                    mlflow_credentials: dict,\n",
    "                    ):\n",
    "    \"\"\"\n",
    "    This objective function trains an XGBoost model given a set of sampled hyperparameters. There is no returned value but a metric that is published to the Optuna study to update the progress of the HPO run.\n",
    "\n",
    "    config: dict, defining the sampled hyperparameters to train the model on.\n",
    "    **The below three parameters are used for nesting each HPO run as a child run**\n",
    "    experiment_name: str, the name of the mlflow experiment to log to. This is inherited from the driver node that initiates the mlflow parent run.\n",
    "    parent_run_id: str, the ID of the parent run. This is inherited from the driver node that initiates the mlflow parent run.\n",
    "    mlflow_credentials: dict, the credentials for logging to mlflow. This is inherited from the driver node. \n",
    "    \"\"\"\n",
    "    # # Set mlflow credentials and active MLflow experiment within each Ray task\n",
    "    os.environ.update(mlflow_credentials)\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "\n",
    "    # Convert to DMatrix format\n",
    "    train_df, test_df = train_dataset.to_pandas(), valid_dataset.to_pandas()\n",
    "    X_train, y_train = train_df.drop(\"target\", axis=1), train_df[\"target\"]\n",
    "    X_test, y_test = test_df.drop(\"target\", axis=1), test_df[\"target\"]\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "    with mlflow.start_run(run_name='xgb_model_hpo_250421', \n",
    "                          nested=True,\n",
    "                          parent_run_id=parent_run_id):\n",
    "\n",
    "        # create dictionary to collect evaluation metrics\n",
    "        evals_result = {}\n",
    "        # Train the classifier\n",
    "        bst = xgb.train(\n",
    "            config,\n",
    "            dtrain,\n",
    "            early_stopping_rounds=config['early_stopping_rounds'],\n",
    "            num_boost_round=1000,\n",
    "            evals=[(dtrain, 'train'), (dtest, 'test')],\n",
    "            evals_result=evals_result,\n",
    "            verbose_eval=10\n",
    "            )\n",
    "\n",
    "        # Retrieve the evaluation metric values from the training process\n",
    "        final_eval_metric = evals_result['test'][config['eval_metric']][-1]\n",
    "        \n",
    "        # write mlflow metrics\n",
    "        mlflow.log_params(config)\n",
    "        mlflow.log_metrics({f'validation_{config[\"eval_metric\"]}': final_eval_metric})\n",
    "\n",
    "    # Return evaluation results back to driver node\n",
    "    train.report({config['eval_metric']: final_eval_metric, \"done\": True})\n",
    "\n",
    "# By default, Ray Tune uses 1 CPU/trial. Since we want to explicitly use one GPU/Optuna trial we'll set that here. With 25M rows x 50 columns, we can train a model with one GPU in about 15 mins. \n",
    "trainable_with_resources = tune.with_resources(train_classifier, \n",
    "                                               {\"gpu\": 1})\n",
    "\n",
    "# Define the hyperparameter search space.\n",
    "param_space = {\n",
    "    \"objective\": \"multi:softmax\",\n",
    "    'eval_metric': 'mlogloss', \n",
    "    \"num_class\": 5,\n",
    "    \"learning_rate\": tune.uniform(0.01, 0.3),\n",
    "    \"n_estimators\": tune.randint(100, 1000),\n",
    "    \"early_stopping_rounds\": tune.randint(3, 20),\n",
    "    \"random_state\": 42,\n",
    "    'tree_method': 'hist',    \n",
    "    'device': 'cuda',    # Use GPU acceleration\n",
    "}\n",
    "\n",
    "# Set up search algorithm. Here we use Optuna and use the default the Bayesian sampler (i.e. TPES)\n",
    "optuna = OptunaSearch(metric=\"mlogloss\", \n",
    "                      mode=\"min\")\n",
    "\n",
    "with mlflow.start_run(run_name ='gpu-run-250422') as parent_run:\n",
    "    tuner = tune.Tuner(\n",
    "        ray.tune.with_parameters(\n",
    "            trainable_with_resources,\n",
    "            experiment_name=experiment_name,\n",
    "            parent_run_id = parent_run.info.run_id,\n",
    "            mlflow_credentials=mlflow_db_creds),\n",
    "        tune_config=tune.TuneConfig(num_samples=num_samples,\n",
    "                                    search_alg=optuna),\n",
    "        param_space=param_space\n",
    "        )\n",
    "    results = tuner.fit()\n",
    "\n",
    "results.get_best_result(metric=\"mlogloss\", \n",
    "                        mode=\"min\").config"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01-single-node-multi-gpu-hpo-xgb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
